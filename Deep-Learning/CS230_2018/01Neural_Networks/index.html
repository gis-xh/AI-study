
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="人工智能个人学习博客">
      
      
        <meta name="author" content="gis-xh">
      
      
        <link rel="canonical" href="https://gis-xh.github.io/AI-study/Deep-Learning/CS230_2018/01Neural_Networks/">
      
      
        <link rel="prev" href="../0102/">
      
      
        <link rel="next" href="../02/">
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.5.2, mkdocs-material-9.1.21">
    
    
      
        <title>Neural Networks and Deep Learning - 人工智能个人学习博客</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.eebd395e.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../css/timeago.css">
    
      <link rel="stylesheet" href="../../../mkdocs/css/unordered-list-symbols.css">
    
      <link rel="stylesheet" href="../../../mkdocs/css/img-center.css">
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
   <link href="../../../assets/stylesheets/glightbox.min.css" rel="stylesheet"/><style>
        html.glightbox-open { overflow: initial; height: 100%; }
        .gslide-title { margin-top: 0px; user-select: text; }
        .gslide-desc { color: #666; user-select: text; }
        .gslide-image img { background: white; }
        
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color);}
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color);}
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color);}
            </style> <script src="../../../assets/javascripts/glightbox.min.js"></script></head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#neural-networks-and-deep-learning" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="人工智能个人学习博客" class="md-header__button md-logo" aria-label="人工智能个人学习博客" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            人工智能个人学习博客
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Neural Networks and Deep Learning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="切换至夜间模式"  type="radio" name="__palette" id="__palette_1">
            
              <label class="md-header__button md-icon" title="切换至夜间模式" for="__palette_2" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3 3.19.09m3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95 2.06.05m-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31Z"/></svg>
              </label>
            
          
            
            
            
            <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="orange"  aria-label="切换至日间模式"  type="radio" name="__palette" id="__palette_2">
            
              <label class="md-header__button md-icon" title="切换至日间模式" for="__palette_1" hidden>
                <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5c-.84 0-1.65.15-2.39.42L12 2M3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29L3.34 7m.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14L3.36 17M20.65 7l-1.77 3.79a7.023 7.023 0 0 0-2.38-4.15l4.15.36m-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29L20.64 17M12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44L12 22Z"/></svg>
              </label>
            
          
        </form>
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="分享" aria-label="分享" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/gis-xh/AI-study" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    gis-xh/AI-study
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../../.." class="md-tabs__link">
      人工智能学习博客
    </a>
  </li>

      
        
  
  
    
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../B%E7%AB%99Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" class="md-tabs__link md-tabs__link--active">
        Deep Learning
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../KG/01/" class="md-tabs__link">
        KG
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../LLM/" class="md-tabs__link">
        LLM
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../../Machine-Learning/00%20%E8%AF%BE%E7%A8%8B%E4%BF%A1%E6%81%AF/" class="md-tabs__link">
        Machine Learning
      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="人工智能个人学习博客" class="md-nav__button md-logo" aria-label="人工智能个人学习博客" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    人工智能个人学习博客
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/gis-xh/AI-study" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
  </div>
  <div class="md-source__repository">
    gis-xh/AI-study
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        人工智能学习博客
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
      
      
        
          
        
          
        
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          Deep Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Deep Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../B%E7%AB%99Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" class="md-nav__link">
        Pytorch 深度学习实践
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" checked>
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../">CS230 2018</a>
          
            <label for="__nav_2_2">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          CS230 2018
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../0102/" class="md-nav__link">
        0102
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Neural Networks and Deep Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Neural Networks and Deep Learning
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1 基本概念
  </a>
  
    <nav class="md-nav" aria-label="1 基本概念">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    1.1 神经网络
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    1.2 监督学习与神经网络
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2 基于神经网络的逻辑回归
  </a>
  
    <nav class="md-nav" aria-label="2 基于神经网络的逻辑回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-binary-classification" class="md-nav__link">
    2.1 Binary Classification 二元分类
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-logistic-regression" class="md-nav__link">
    2.2 Logistic Regression 逻辑回归模型
  </a>
  
    <nav class="md-nav" aria-label="2.2 Logistic Regression 逻辑回归模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221" class="md-nav__link">
    2.2.1 逻辑回归模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-loss" class="md-nav__link">
    2.2.2 Loss 损失函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-cost" class="md-nav__link">
    2.2.3 Cost 成本函数
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-gradient-descent" class="md-nav__link">
    2.3 Gradient Descent 梯度下降算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-computation-graph" class="md-nav__link">
    2.4 Computation Graph 计算流程图
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25" class="md-nav__link">
    2.5 逻辑回归的梯度下降
  </a>
  
    <nav class="md-nav" aria-label="2.5 逻辑回归的梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#251" class="md-nav__link">
    2.5.1 单个样本的损失函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#252" class="md-nav__link">
    2.5.2 整体样本的成本函数
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26-vectorization" class="md-nav__link">
    2.6 Vectorization 向量化
  </a>
  
    <nav class="md-nav" aria-label="2.6 Vectorization 向量化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#261" class="md-nav__link">
    2.6.1 基础理论
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#262" class="md-nav__link">
    2.6.2 向量化计算逻辑回归预测值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#263" class="md-nav__link">
    2.6.3 向量化同时计算逻辑回归的梯度
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27-broadcasting" class="md-nav__link">
    2.7 Broadcasting 广播
  </a>
  
    <nav class="md-nav" aria-label="2.7 Broadcasting 广播">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#271-broadcasting" class="md-nav__link">
    2.7.1 Broadcasting 代码示例
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#272-broadcasting" class="md-nav__link">
    2.7.2 Broadcasting 常用原则
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#28-python" class="md-nav__link">
    2.8 Python 向量编码技巧
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3 浅层神经网络
  </a>
  
    <nav class="md-nav" aria-label="3 浅层神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    3.1 单隐藏层神经网络结构
  </a>
  
    <nav class="md-nav" aria-label="3.1 单隐藏层神经网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#311" class="md-nav__link">
    3.1.1 网络结构图解
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#312" class="md-nav__link">
    3.1.2 网络输出原理
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    3.2 向量化样本
  </a>
  
    <nav class="md-nav" aria-label="3.2 向量化样本">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321" class="md-nav__link">
    3.2.1 单样本向量化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322" class="md-nav__link">
    3.2.2 多样本向量化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323" class="md-nav__link">
    3.2.3 前向传播向量化步骤
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#324" class="md-nav__link">
    3.2.4 向量化证明
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    3.3 激活函数
  </a>
  
    <nav class="md-nav" aria-label="3.3 激活函数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#331" class="md-nav__link">
    3.3.1 为什么使用非线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#332-sigmoid" class="md-nav__link">
    3.3.2 sigmoid 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#333-tanhz" class="md-nav__link">
    3.3.3 \(\tanh(z)\) 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#334-relu" class="md-nav__link">
    3.3.4 ReLU 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#335-leaky-relu" class="md-nav__link">
    3.3.5 Leaky ReLU 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#336" class="md-nav__link">
    3.3.6 激活函数的选择
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-" class="md-nav__link">
    3.4 后向传播 - 梯度下降
  </a>
  
    <nav class="md-nav" aria-label="3.4 后向传播 - 梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#341" class="md-nav__link">
    3.4.1 单个样本的梯度计算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#342" class="md-nav__link">
    3.4.2 多样本梯度计算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#343" class="md-nav__link">
    3.4.3 学习率控制梯度下降
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35" class="md-nav__link">
    3.5 随机初始化权重
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4 深层神经网络
  </a>
  
    <nav class="md-nav" aria-label="4 深层神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    4.1 基本概念
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    4.2 确保维度正确
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    4.3 构建神经网络结构
  </a>
  
    <nav class="md-nav" aria-label="4.3 构建神经网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#431" class="md-nav__link">
    4.3.1 正向传播
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#432" class="md-nav__link">
    4.3.2 反向传播
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44" class="md-nav__link">
    4.4 超参数调优
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02/" class="md-nav__link">
        Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Lecture2/" class="md-nav__link">
        Lecture2
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_3" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          KG
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          KG
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../KG/01/" class="md-nav__link">
        知识图谱 Knowledge Graph
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../LLM/">LLM</a>
          
            <label for="__nav_4">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          LLM
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/1Pre-Train-Language-Model/" class="md-nav__link">
        预训练语言模型基础理论
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/2Transformer/" class="md-nav__link">
        Transformer 框架理论
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/3GPT%2BBERT/" class="md-nav__link">
        GPT 与 BERT 模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/5LangChain/" class="md-nav__link">
        5LangChain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/6ChatGLM%2BLangChain/" class="md-nav__link">
        ChatGLM+LangChain
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/RWKV/" class="md-nav__link">
        RWKV
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/prompt_template/" class="md-nav__link">
        Prompt template
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/vectorstore/" class="md-nav__link">
        向量数据库
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%80%9D%E8%B7%AF/" class="md-nav__link">
        基于现有语言模型进行微调
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_11" >
      
      
        
          
            
          
        
          
        
          
        
          
        
          
        
          
        
      
      
        
        
        <div class="md-nav__link md-nav__link--index ">
          <a href="../../../LLM/Deeplearning.AI/">Deeplearning.AI</a>
          
            <label for="__nav_4_11">
              <span class="md-nav__icon md-icon"></span>
            </label>
          
        </div>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_11_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_11">
          <span class="md-nav__icon md-icon"></span>
          Deeplearning.AI
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/Deeplearning.AI/1Prompt_Engineering/" class="md-nav__link">
        ChatGPT Prompt Engineering for Developers
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/Deeplearning.AI/2ChatGPT_API/" class="md-nav__link">
        Building Systems with the ChatGPT API
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/Deeplearning.AI/3LangChain_LLM_Application/" class="md-nav__link">
        LangChain for LLM Application Development
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/Deeplearning.AI/4LangChain_Data/" class="md-nav__link">
        LangChain Chat With Your Data
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../LLM/Deeplearning.AI/5Gradio_Generative_AI/" class="md-nav__link">
        Building Generative AI Applications with Gradio
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
        
      
      <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
      
      
        
          
        
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Machine Learning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Machine Learning
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Machine-Learning/00%20%E8%AF%BE%E7%A8%8B%E4%BF%A1%E6%81%AF/" class="md-nav__link">
        吴恩达机器学习系列课程学习记录（一）
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1" class="md-nav__link">
    1 基本概念
  </a>
  
    <nav class="md-nav" aria-label="1 基本概念">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    1.1 神经网络
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    1.2 监督学习与神经网络
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2" class="md-nav__link">
    2 基于神经网络的逻辑回归
  </a>
  
    <nav class="md-nav" aria-label="2 基于神经网络的逻辑回归">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-binary-classification" class="md-nav__link">
    2.1 Binary Classification 二元分类
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-logistic-regression" class="md-nav__link">
    2.2 Logistic Regression 逻辑回归模型
  </a>
  
    <nav class="md-nav" aria-label="2.2 Logistic Regression 逻辑回归模型">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221" class="md-nav__link">
    2.2.1 逻辑回归模型
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-loss" class="md-nav__link">
    2.2.2 Loss 损失函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#223-cost" class="md-nav__link">
    2.2.3 Cost 成本函数
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-gradient-descent" class="md-nav__link">
    2.3 Gradient Descent 梯度下降算法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-computation-graph" class="md-nav__link">
    2.4 Computation Graph 计算流程图
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25" class="md-nav__link">
    2.5 逻辑回归的梯度下降
  </a>
  
    <nav class="md-nav" aria-label="2.5 逻辑回归的梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#251" class="md-nav__link">
    2.5.1 单个样本的损失函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#252" class="md-nav__link">
    2.5.2 整体样本的成本函数
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#26-vectorization" class="md-nav__link">
    2.6 Vectorization 向量化
  </a>
  
    <nav class="md-nav" aria-label="2.6 Vectorization 向量化">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#261" class="md-nav__link">
    2.6.1 基础理论
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#262" class="md-nav__link">
    2.6.2 向量化计算逻辑回归预测值
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#263" class="md-nav__link">
    2.6.3 向量化同时计算逻辑回归的梯度
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#27-broadcasting" class="md-nav__link">
    2.7 Broadcasting 广播
  </a>
  
    <nav class="md-nav" aria-label="2.7 Broadcasting 广播">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#271-broadcasting" class="md-nav__link">
    2.7.1 Broadcasting 代码示例
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#272-broadcasting" class="md-nav__link">
    2.7.2 Broadcasting 常用原则
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#28-python" class="md-nav__link">
    2.8 Python 向量编码技巧
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3" class="md-nav__link">
    3 浅层神经网络
  </a>
  
    <nav class="md-nav" aria-label="3 浅层神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    3.1 单隐藏层神经网络结构
  </a>
  
    <nav class="md-nav" aria-label="3.1 单隐藏层神经网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#311" class="md-nav__link">
    3.1.1 网络结构图解
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#312" class="md-nav__link">
    3.1.2 网络输出原理
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32" class="md-nav__link">
    3.2 向量化样本
  </a>
  
    <nav class="md-nav" aria-label="3.2 向量化样本">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321" class="md-nav__link">
    3.2.1 单样本向量化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322" class="md-nav__link">
    3.2.2 多样本向量化
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323" class="md-nav__link">
    3.2.3 前向传播向量化步骤
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#324" class="md-nav__link">
    3.2.4 向量化证明
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    3.3 激活函数
  </a>
  
    <nav class="md-nav" aria-label="3.3 激活函数">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#331" class="md-nav__link">
    3.3.1 为什么使用非线性激活函数？
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#332-sigmoid" class="md-nav__link">
    3.3.2 sigmoid 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#333-tanhz" class="md-nav__link">
    3.3.3 \(\tanh(z)\) 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#334-relu" class="md-nav__link">
    3.3.4 ReLU 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#335-leaky-relu" class="md-nav__link">
    3.3.5 Leaky ReLU 函数及其导数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#336" class="md-nav__link">
    3.3.6 激活函数的选择
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-" class="md-nav__link">
    3.4 后向传播 - 梯度下降
  </a>
  
    <nav class="md-nav" aria-label="3.4 后向传播 - 梯度下降">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#341" class="md-nav__link">
    3.4.1 单个样本的梯度计算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#342" class="md-nav__link">
    3.4.2 多样本梯度计算
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#343" class="md-nav__link">
    3.4.3 学习率控制梯度下降
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35" class="md-nav__link">
    3.5 随机初始化权重
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    4 深层神经网络
  </a>
  
    <nav class="md-nav" aria-label="4 深层神经网络">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41" class="md-nav__link">
    4.1 基本概念
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42" class="md-nav__link">
    4.2 确保维度正确
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43" class="md-nav__link">
    4.3 构建神经网络结构
  </a>
  
    <nav class="md-nav" aria-label="4.3 构建神经网络结构">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#431" class="md-nav__link">
    4.3.1 正向传播
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#432" class="md-nav__link">
    4.3.2 反向传播
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44" class="md-nav__link">
    4.4 超参数调优
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                

                  
  



  
  


<h1 id="neural-networks-and-deep-learning">Neural Networks and Deep Learning<a class="headerlink" href="#neural-networks-and-deep-learning" title="Permanent link">&para;</a></h1>
<p>官网视频课程：https://www.coursera.org/learn/neural-networks-deep-learning/lecture/</p>
<h2 id="1">1 基本概念<a class="headerlink" href="#1" title="Permanent link">&para;</a></h2>
<h3 id="11">1.1 神经网络<a class="headerlink" href="#11" title="Permanent link">&para;</a></h3>
<p><strong>1、深度学习：</strong>一般指的是训练神经网络，有时是非常非常大（深）的神经网络。</p>
<p><strong>2、Sigmoid 函数：</strong></p>
<div class="arithmatex">\[
\sigma(z)=\frac{1}{1+e^{-z}}, \ \ z\in R
\]</div>
<ul>
<li>从 0 平滑地升高到 1，但是在两端梯度为 0，不利于后续计算</li>
</ul>
<p><a class="glightbox" href="../img/image-20230729165611899.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230729165611899" src="../img/image-20230729165611899.png" /></a></p>
<p><center>图 1.1.1 Sigmoid 函数图</center></p>
<p><strong>3、ReLU (Rectified Linerar Unit) 函数：</strong>修正线性单元函数，可以在单一神经元内所运行的函数。</p>
<p>深度学习的一大重点突破就是将激活函数从 sigmoid 函数迁移到 ReLU 函数，这样可以很好地提高梯度下降算法的速度。</p>
<p><strong>4、神经元：</strong>在其内部可以运行一个函数，比如 ReLU 函数，其他非线性函数</p>
<p><a class="glightbox" href="../img/image-20230728114716764.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230728114716764" src="../img/image-20230728114716764.png" /></a></p>
<p><center>图 1.1.2 房价预测示例图</center></p>
<p><strong>5、神经网络：</strong>由许多单一神经元叠加组成</p>
<p><a class="glightbox" href="../img/image-20230728115745077.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230728115745077" src="../img/image-20230728115745077.png" /></a></p>
<p><center>图 1.1.3 房价预测中的简易神经网络示例</center></p>
<ul>
<li>给定足够多的训练示例 x 和 y，神经网络就能够很好地拟合出一个函数来建立 x 与 y 之间的映射关系。</li>
</ul>
<p><a class="glightbox" href="../img/image-20230728120916923.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230728120916923" src="../img/image-20230728120916923.png" /></a></p>
<p><center>图 1.1.4 房价预测中的简易神经网络抽象化</center></p>
<h3 id="12">1.2 监督学习与神经网络<a class="headerlink" href="#12" title="Permanent link">&para;</a></h3>
<p><strong>1、监督学习：</strong>把一个输入的 x 与一个输出的 y 相对应起来，神经网络与监督学习密不可分。</p>
<p><strong>2、几种神经网络</strong></p>
<ul>
<li>StandNN：标准神经网络，用于广告推荐，房价预测……</li>
<li>CNN：卷积神经网络，常用于图像数据，如图像分类……</li>
<li>RNN：循环神经网络，常用于序列化数据，应用于语言翻译和录音识别……</li>
</ul>
<p><a class="glightbox" href="../img/image-20230728230111641.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230728230111641" src="../img/image-20230728230111641.png" /></a></p>
<p><center>图 1.2.1 三种神经网络示例图</center></p>
<p><strong>3、结构化数据</strong></p>
<p>对于每一个特征都有清晰的定义，常用的有数据库表。</p>
<p><strong>4、非结构化数据</strong></p>
<p>非结构化数据主要是音频、图像、文本等数据，对于这些数据计算机很难理解，但是人类却能够很好地理解。深度学习通过神经网络能够更好地解释非结构化数据，例如语音识别、图像识别、NLP……</p>
<ul>
<li>图像像素值</li>
<li>文本中的独立单词</li>
</ul>
<p><a class="glightbox" href="../img/image-20230728230303030.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230728230303030" src="../img/image-20230728230303030.png" /></a></p>
<p><center>图 1.2.2 非结构化数据示例图</center></p>
<p><strong>5、神经网络与其他算法的比较</strong></p>
<ul>
<li>在较小的训练数据集中，神经网络的优势可能并不明显；当训练数据集很大时，神经网络的优势将会凸显</li>
<li>大、中、小型神经网络</li>
<li>传统机器学习算法：SVM 支持向量机、Logistic Regression 逻辑回归、……</li>
</ul>
<p><a class="glightbox" href="../img/image-20230729111124715.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230729111124715" src="../img/image-20230729111124715.png" /></a></p>
<p><center>图 1.2.3 不同算法在不同数据规模的表现</center></p>
<p>1、Data：庞大的数据规模</p>
<p>2、Computation：高效的设备算力，提高计算速度</p>
<p>3、Algorithms：算法优化创新</p>
<h2 id="2">2 基于神经网络的逻辑回归<a class="headerlink" href="#2" title="Permanent link">&para;</a></h2>
<h3 id="21-binary-classification">2.1 Binary Classification 二元分类<a class="headerlink" href="#21-binary-classification" title="Permanent link">&para;</a></h3>
<p><strong>1、二元分类</strong></p>
<ul>
<li>对于输入的数据，经过计算，输出一个 0，1 标签的结果</li>
<li>例如将图片中的 RGB 色彩矩阵作为输入数据，经过计算，辨别图像是否有猫，有为 1，无则为 0。</li>
</ul>
<p><strong>2、单个样本 <span class="arithmatex">\((x, y)\)</span></strong></p>
<ul>
<li>x：一个 <span class="arithmatex">\(n(x)\)</span> 维的特征向量</li>
<li>y：label 标签，<span class="arithmatex">\(y\in\{0, 1\}\)</span></li>
</ul>
<p><strong>3、训练集</strong></p>
<ul>
<li><span class="arithmatex">\(\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \cdots, (x^{(m)}, y^{(m)})\}\)</span></li>
<li><span class="arithmatex">\(m=M_{train}\)</span>：训练集的样本总数</li>
<li><span class="arithmatex">\((x^{(1)}, y^{(1)})\)</span>：第一个样本的输入与输出</li>
</ul>
<p><strong>4、训练集的输入矩阵 <span class="arithmatex">\(X\)</span></strong></p>
<div class="arithmatex">\[
X=
\begin{bmatrix}
  |&amp;  |&amp;  |&amp;  |\\
  |&amp;  |&amp;  |&amp;  |\\
  x^{(1)}&amp;  x^{(2)}&amp;  \cdots&amp;  x^{(m)}\\
  |&amp;  |&amp;  |&amp;  |\\
  |&amp;  |&amp;  |&amp;  |
\end{bmatrix}
\]</div>
<ul>
<li>将训练样本的输入 <strong><u>按列排列</u></strong> 的形式（按列会简单一些）</li>
<li><code>X.shape = (nx, m)</code>：X 是一个 <span class="arithmatex">\(n_x\)</span> 行，<span class="arithmatex">\(m\)</span> 列的矩阵</li>
</ul>
<p><strong>5、训练集的输出矩阵 <span class="arithmatex">\(Y\)</span></strong></p>
<div class="arithmatex">\[
Y=[y^{(1)}, y^{(2)}, \cdots, y^{(m)}]
\]</div>
<ul>
<li><code>Y.shape = (1, m)</code>：Y 是一个 1 行，<span class="arithmatex">\(m\)</span> 列的矩阵</li>
</ul>
<h3 id="22-logistic-regression">2.2 Logistic Regression 逻辑回归模型<a class="headerlink" href="#22-logistic-regression" title="Permanent link">&para;</a></h3>
<h4 id="221">2.2.1 逻辑回归模型<a class="headerlink" href="#221" title="Permanent link">&para;</a></h4>
<p>给定一个训练数据集 <span class="arithmatex">\(\{(x^{(1)}, y^{(1)}), \cdots, (x^{(m)}, y^{(m)})\}\)</span>，想要得到 <span class="arithmatex">\(y^{(i)}\approx \hat{y}^{(i)}\)</span> 的结果。</p>
<div class="arithmatex">\[
\begin{align*}
\hat{y}^{(i)}&amp;=P(y=1|\ x^{(i)}) \\\\
&amp;=\sigma(z^{(i)})=\frac{1}{1+e^{-z^{(i)}}} \\\\
&amp;=\sigma(w^T x^{(i)}+b)
\end{align*}
\]</div>
<ul>
<li>
<p><span class="arithmatex">\(\hat{y}\)</span>：预测图片是猫的图片的概率</p>
</li>
<li>
<p>相关参数：<span class="arithmatex">\(w\)</span> 是一个维度为 <span class="arithmatex">\(n^{(x)}\)</span> 的向量，<span class="arithmatex">\(b\)</span> 是一个实数</p>
</li>
</ul>
<h4 id="222-loss">2.2.2 Loss 损失函数<a class="headerlink" href="#222-loss" title="Permanent link">&para;</a></h4>
<p>损失函数计算单个训练样例的误差，是针对单个训练样例定义的函数，用于衡量在单个训练样本上的表现。</p>
<ul>
<li>
<p>（半）平方误差：<span class="arithmatex">\(loss(\hat{y},y)=\frac{1}{2}(\hat{y}-y)^2\)</span></p>
</li>
<li>
<p>应用于逻辑回归的损失函数：</p>
</li>
</ul>
<p><span class="arithmatex">\(loss(\hat{y},y)=-[y^{(i)} \ln^{\hat{y}^{(i)}}+(1-y^{(i)}) \ln^{(1-\hat{y}^{(i)})}]\)</span></p>
<h4 id="223-cost">2.2.3 Cost 成本函数<a class="headerlink" href="#223-cost" title="Permanent link">&para;</a></h4>
<p>代价函数是整个训练集损失函数的平均值，是针对整个训练样例定义的函数，用于衡量在整个训练集上的表现。是一个凸函数 (convex function)，能够找到最低点，得到局部最优解 (local optima)。</p>
<div class="arithmatex">\[
\begin{align*}
J(w,b)&amp;=\frac{1}{m}\sum_{i=1}^{m}loss(\hat{y}^{(i)}, y^{(i)}) \\\\
&amp;=-\frac{1}{m}\sum_{i=1}^{m}[y^{(i)} \ln^{\hat{y}^{(i)}}+(1-y^{(i)}) \ln^{(1-\hat{y}^{(i)})}]
\end{align*}
\]</div>
<h3 id="23-gradient-descent">2.3 Gradient Descent 梯度下降算法<a class="headerlink" href="#23-gradient-descent" title="Permanent link">&para;</a></h3>
<p><strong>1、梯度下降：</strong>找到 <span class="arithmatex">\(w, \ b\)</span> 使得成本函数 <span class="arithmatex">\(J(w,b)\)</span> 的结果尽可能地小，以此来减小误差。</p>
<p><strong>2、渐变梯度下降算法：</strong>通过不断迭代，将初始点向最陡的下坡方向（下降最快的方向）前进，直到到达最低点。</p>
<p><a class="glightbox" href="../img/image-20230729191159408.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230729191159408" src="../img/image-20230729191159408.png" /></a></p>
<p><center>图 2.1 梯度下降示意图</center></p>
<p><strong>3、斜率与最优解</strong></p>
<ul>
<li>derivative 导数，slope 斜率，partial derivative 偏导数</li>
<li>对于成本函数 <span class="arithmatex">\(J(w,b)\)</span> 迭代求出最优解</li>
<li>这里的偏导符号 <span class="arithmatex">\(\partial\)</span>，在实际编程中作为积分符号 <span class="arithmatex">\(d\)</span> 处理，<span class="arithmatex">\(\alpha\)</span> 是学习率</li>
</ul>
<div class="arithmatex">\[
w:=w-\alpha \frac{\partial J(w,b)}{\partial w}=w-\alpha dw \\\\
b:=b-\alpha \frac{\partial J(w,b)}{\partial b}=b-\alpha db
\]</div>
<h3 id="24-computation-graph">2.4 Computation Graph 计算流程图<a class="headerlink" href="#24-computation-graph" title="Permanent link">&para;</a></h3>
<p>神经网络中的计算即是由多个计算网络输出的前向传播与计算梯度的后向传播构成。对于函数 <span class="arithmatex">\(J(a,b,b)=3(a+bc)\)</span></p>
<p><strong>1、前向传播（Forward Propagation）：</strong>按顺序计算代价函数。</p>
<p><a class="glightbox" href="../img/image-20230729195918986.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230729195918986" src="../img/image-20230729195918986.png" /></a></p>
<p><center>图 2.2 前向传播计算流程图</center></p>
<p><strong>2、反向传播（Backward Propagation）：</strong>当需要计算最终值相对于某个特征变量的导数时，需要利用计算图中上一步的结点定义。（遵循积分中的链式法则）</p>
<div class="arithmatex">\[
\frac{dJ}{db}=\frac{dJ}{dv}\cdot\frac{dv}{du}\cdot\frac{du}{db}
\]</div>
<p><a class="glightbox" href="../img/image-20230729202013657.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230729202013657" src="../img/image-20230729202013657.png" /></a></p>
<p><center>图 2.3 反向传播计算流程图（红）</center></p>
<p><strong>3、代码规定</strong></p>
<ul>
<li>在 Python 代码中，为了减少变量名，约定了 <span class="arithmatex">\(dvar\)</span> 作为积分的结果，它代表着最终输出变量相对于各种中间量的导数。</li>
</ul>
<div class="arithmatex">\[
dvar=\frac{dFinalOutputVar}{dvar}
\]</div>
<h3 id="25">2.5 逻辑回归的梯度下降<a class="headerlink" href="#25" title="Permanent link">&para;</a></h3>
<h4 id="251">2.5.1 单个样本的损失函数<a class="headerlink" href="#251" title="Permanent link">&para;</a></h4>
<p>对于逻辑回归，相关公式如下：</p>
<div class="arithmatex">\[
\begin{align*}
&amp;z = w^T x + b \\\\
&amp;\hat{y} = a = \sigma(z) = \frac{1}{1 + e^{-z}} \\\\
&amp;L(a, y) = -(y \ln^{(a)} + (1 - y) \ln^{(1 - a)})
\end{align*}
\]</div>
<ul>
<li>对于单一样本前向传播计算损失函数。</li>
</ul>
<p><a class="glightbox" href="../img/image-20230729223442615.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230729223442615" src="../img/image-20230729223442615.png" /></a></p>
<p><center>图 2.4 逻辑回归前向传播</center></p>
<ul>
<li>若计算损失函数关于 z 的导数，需要通过后向传播（链式法则）计算导数</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
\frac{dL(a,y)}{dz}
=\frac{dL(a,y)}{da}\cdot\frac{da}{dz} 
=\frac{a-y}{a(1-a)}\cdot a(1-a)
=a-y
\end{align*}
\]</div>
<h4 id="252">2.5.2 整体样本的成本函数<a class="headerlink" href="#252" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
\begin{align*}
J(w,b)&amp;=\frac{1}{m}\sum_{i=1}^{m}L(a^{(i)}, y^{(i)}) \\\\
dw_1=\frac{\partial {J(w,b)}}{\partial w_1}&amp;=\frac{1}{m}\sum_{i=1}^{m}\frac{\partial}{\partial w_1}L(a^{(i)}, y^{(i)}) \\\\
&amp;=\frac{1}{m}\sum_{i=1}^{m}dw_{1}^{(i)}
\end{align*}
\]</div>
<h3 id="26-vectorization">2.6 Vectorization 向量化<a class="headerlink" href="#26-vectorization" title="Permanent link">&para;</a></h3>
<h4 id="261">2.6.1 基础理论<a class="headerlink" href="#261" title="Permanent link">&para;</a></h4>
<p><strong>1、向量化：</strong>在深度学习中，一定要避免使用 for loop，而是改用向量化计算，这样可以加快计算的速度。</p>
<p><a class="glightbox" href="../img/image-20230730143753318.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230730143753318" src="../img/image-20230730143753318.png" /></a></p>
<p><center>图 2.5 向量计算与 For 循环计算时间对比</center></p>
<p><strong>2、SIMD 指令：</strong>Single Instruction Multiple Data，代表了一种使用 <strong>单条指令</strong> 对 <strong>多个数据</strong> 进行 <strong>并行操作</strong> 的技术。如果使用了 (build-in) 内置函数，像这样 <code>np.function</code> 或者不需要实现循环的函数，这使得 python 中的 numpy 能够充分利用 SIMD 指令去更快地计算，GPU 被认为更加擅长 SIMD 计算。</p>
<p><strong>3、build-in 内置函数</strong>：</p>
<p>在使用 for 循环前，先查看是否可以用内置函数避免使用 for 循环。</p>
<ul>
<li><code>np.exp(v)</code>：对向量中每个元素求 e</li>
<li><code>np.log(v)</code>：对向量中每个元素求对数</li>
<li><code>np.abs(v)</code>：对向量中每个元素求绝对值</li>
</ul>
<h4 id="262">2.6.2 向量化计算逻辑回归预测值<a class="headerlink" href="#262" title="Permanent link">&para;</a></h4>
<p>1、将输入数据的 <span class="arithmatex">\(x^{(1)}, x^{(2)}, \cdots, x^{(m)}\)</span> 组合成矩阵 <span class="arithmatex">\(X\)</span></p>
<div class="arithmatex">\[
X=
\begin{bmatrix}
  |&amp;  |&amp;  |&amp;  |\\
  |&amp;  |&amp;  |&amp;  |\\
  x^{(1)}&amp;  x^{(2)}&amp;  \cdots&amp;  x^{(m)}\\
  |&amp;  |&amp;  |&amp;  |\\
  |&amp;  |&amp;  |&amp;  |
\end{bmatrix}
\]</div>
<p>2、参数 <span class="arithmatex">\(w\)</span> 是一个 m 维的列向量，将参数  <span class="arithmatex">\(b\)</span> 扩展成 m 维的行向量（在实际操作中，当出现向量+常量的情况时，Python 会自动将常量扩展成向量）</p>
<p>3、将公式 <span class="arithmatex">\(z=w^Tx+b\)</span>，变成如下形式：</p>
<div class="arithmatex">\[
\begin{align*}
Z&amp;=[z^{(1)}, z^{(2)}, \cdots, z^{(m)}]=w^TX+[b,b,\cdots,b] \\\\
&amp;=[w^Tx^{(1)}+b, w^Tx^{(2)}+b,\cdots,w^Tx^{(m)}+b]
\end{align*}
\]</div>
<ul>
<li>在 Numpy 中对应代码 <code>Z = np.dot(wT, x) + b</code></li>
</ul>
<p>4、将预测值也使用向量表示</p>
<div class="arithmatex">\[
A=[\hat{y}^{(1)},\hat{y}^{(2)},\cdots,\hat{y}^{(m)}]=[a^{(1)}, a^{(2)}, \cdots, a^{(m)}]=\sigma(Z)
\]</div>
<h4 id="263">2.6.3 向量化同时计算逻辑回归的梯度<a class="headerlink" href="#263" title="Permanent link">&para;</a></h4>
<p>1、将单个样本的梯度值 <span class="arithmatex">\(dz^{(1)}=a^{(1)}-y^{(1)}\)</span> 组合成向量 <span class="arithmatex">\(dZ\)</span></p>
<div class="arithmatex">\[
\begin{align*}
dZ&amp;=[dz^{(1)},dz^{(2)},\cdots,dz^{(m)}] \\\\
&amp;=A-Y
=[a^{(1)}-y^{(1)},a^{(2)}-y^{(2)},\cdots,a^{(m)}-y^{(m)}]
\end{align*}
\]</div>
<p>2、关于 <span class="arithmatex">\(b\)</span> 的求导 <span class="arithmatex">\(db\)</span>，可以表示为：</p>
<div class="arithmatex">\[
db=\frac{1}{m}\sum_{i=1}^{m}dz^{(i)}
=\frac{1}{m}np.sum(dZ)
\]</div>
<p>3、关于 <span class="arithmatex">\(w\)</span> 的求导 <span class="arithmatex">\(dw\)</span>，可以表示为：</p>
<div class="arithmatex">\[
dw=\frac{1}{m}X{dZ}^T
\]</div>
<p>4、更新梯度下降参数</p>
<div class="arithmatex">\[
w:=w-\alpha dw \\
b:=b-\alpha db
\]</div>
<p>5、为了使梯度迭代指定的次数，需要在上述步骤中整合到 for 循环中。</p>
<h3 id="27-broadcasting">2.7 Broadcasting 广播<a class="headerlink" href="#27-broadcasting" title="Permanent link">&para;</a></h3>
<h4 id="271-broadcasting">2.7.1 Broadcasting 代码示例<a class="headerlink" href="#271-broadcasting" title="Permanent link">&para;</a></h4>
<p><strong>1、创建矩阵</strong></p>
<ul>
<li>创建一个 3 × 4 的矩阵</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">56.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">,</span><span class="mf">4.4</span><span class="p">,</span><span class="mf">68.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.2</span><span class="p">,</span><span class="mf">104.0</span><span class="p">,</span><span class="mf">52.0</span><span class="p">,</span><span class="mf">8.0</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">1.8</span><span class="p">,</span><span class="mf">135.0</span><span class="p">,</span><span class="mf">99.0</span><span class="p">,</span><span class="mf">0.9</span><span class="p">]])</span>
</code></pre></div>
<p><strong>2、按列求和</strong></p>
<ul>
<li>对每一列进行求和，<code>axis=0</code> 表示垂直方向求和，得到一个 1×4 的矩阵</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">cal</span> <span class="o">=</span> <span class="n">A</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>
<p><strong>3、计算百分比</strong></p>
<ul>
<li>reshape 操作：在不确定矩阵维数时，reshape 的调用成本很低，可以使用 <code>np.reshape(m,n)</code>  来指定行列数。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="n">percentage</span> <span class="o">=</span> <span class="mi">100</span><span class="o">*</span><span class="n">A</span><span class="o">/</span><span class="n">cal</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div>
<p><strong>4、打印输出</strong></p>
<div class="highlight"><pre><span></span><code><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;矩阵A: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">A</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;每一列总和: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">cal</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;每项占比: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">percentage</span><span class="p">)</span>
</code></pre></div>
<p><a class="glightbox" href="../img/image-20230730170652499.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230730170652499" src="../img/image-20230730170652499.png" /></a></p>
<p><center>图 2.6 Broadcasting 示例输出</center></p>
<h4 id="272-broadcasting">2.7.2 Broadcasting 常用原则<a class="headerlink" href="#272-broadcasting" title="Permanent link">&para;</a></h4>
<ul>
<li>向量 + 常量时，Python 会自动将常量扩展为向量</li>
</ul>
<div class="arithmatex">\[
\begin{bmatrix}
 1\\
 2\\
 3
\end{bmatrix}
+100=
\begin{bmatrix}
 1\\
 2\\
 3
\end{bmatrix}
+
\begin{bmatrix}
 100\\
 100\\
 100
\end{bmatrix}
=
\begin{bmatrix}
 101\\
 102\\
 103
\end{bmatrix}
\]</div>
<ul>
<li><span class="arithmatex">\(m\times n\)</span> 的矩阵加减乘除上 <span class="arithmatex">\(1\times n\)</span> 的矩阵，Python 会自动将后者复制 <span class="arithmatex">\(m-1\)</span> 次，使其也变为一个 <span class="arithmatex">\(m\times n\)</span> 的矩阵</li>
</ul>
<div class="arithmatex">\[
\begin{bmatrix}
  1&amp; 2\\
  3&amp; 4
\end{bmatrix}
+
\begin{bmatrix}
  100&amp; 200
\end{bmatrix}
=
\begin{bmatrix}
  1&amp; 2\\
  3&amp; 4
\end{bmatrix}
+
\begin{bmatrix}
  100&amp; 200\\
  100&amp; 200
\end{bmatrix}
=
\begin{bmatrix}
  101&amp; 202\\
  103&amp; 204
\end{bmatrix}
\]</div>
<ul>
<li>同理，<span class="arithmatex">\(m\times n\)</span> 的矩阵加减乘除上 <span class="arithmatex">\(m\times 1\)</span> 的矩阵，Python 会自动将后者复制 <span class="arithmatex">\(n-1\)</span> 次，使其也变为一个 <span class="arithmatex">\(m\times n\)</span> 的矩阵</li>
</ul>
<div class="arithmatex">\[
\begin{bmatrix}
  1&amp; 2\\
  3&amp; 4
\end{bmatrix}
+
\begin{bmatrix}
  100\\
  200
\end{bmatrix}
=
\begin{bmatrix}
  1&amp; 2\\
  3&amp; 4
\end{bmatrix}
+
\begin{bmatrix}
  100&amp; 100\\
  200&amp; 200
\end{bmatrix}
=
\begin{bmatrix}
  101&amp; 102\\
  203&amp; 204
\end{bmatrix}
\]</div>
<h3 id="28-python">2.8 Python 向量编码技巧<a class="headerlink" href="#28-python" title="Permanent link">&para;</a></h3>
<p><strong>1、不要使用秩为 1 的数组</strong></p>
<ul>
<li>在处理秩为 1 的矩阵时，可以先将使用 <code>reshape</code> 其转化为一个列向量</li>
<li>在生成随机数组时，可以先指定数组的形状，不要使用秩为 1 的数组</li>
</ul>
<p><strong>2、使用 assert 复查矩阵和数组的维度</strong></p>
<ul>
<li>强制让矩阵变为一个 5×1 的列向量</li>
<li><code>assert()</code> 的运行成本与 <code>reshape</code> 一样都很低，不需要考虑成本问题。</li>
</ul>
<div class="highlight"><pre><span></span><code><span class="k">assert</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="o">==</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div>
<h2 id="3">3 浅层神经网络<a class="headerlink" href="#3" title="Permanent link">&para;</a></h2>
<h3 id="31">3.1 单隐藏层神经网络结构<a class="headerlink" href="#31" title="Permanent link">&para;</a></h3>
<h4 id="311">3.1.1 网络结构图解<a class="headerlink" href="#311" title="Permanent link">&para;</a></h4>
<p>下面是一个简单的神经网络结构图，它是双层神经网络，包括一个隐藏层和一个输出层。（一般计算分层时不包括输入层）</p>
<p><a class="glightbox" href="../img/image-20230801203338869.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230801203338869" src="../img/image-20230801203338869.png" /></a></p>
<p><center>图 3.1 含有一个隐藏层的神经网络</center></p>
<p><strong>1、输入层</strong>：将输入数据以矩阵/向量的形式表示</p>
<div class="arithmatex">\[
a^{[0]}=x^{(1)}=\begin{bmatrix}
x^{(1)}_1\\
x^{(1)}_2\\
x^{(1)}_3
\end{bmatrix},\
x^{(1)}\in \mathbb{R}^{3\times 1}
\]</div>
<p><strong>2、隐藏层</strong>：把向量 <span class="arithmatex">\(x^{(1)}\)</span> 作为输入计算激活函数，一个节点生成一个值</p>
<ul>
<li>隐藏层是指在训练集中间这一层节点的真实值，无法被直接观察</li>
<li>有两个相关参数：权重 <span class="arithmatex">\(w^{[1]}\in \mathbb{R}^{4\times 3}\)</span>，其中行数：4（4 个结点），列数：3（3 个输入值），以及偏差 <span class="arithmatex">\(b^{[1]}\in \mathbb{R}^{4\times 1}\)</span></li>
</ul>
<div class="arithmatex">\[
a^{[1]}=\begin{bmatrix}
a^{[1]}_1
\\
a^{[1]}_2
\\
a^{[1]}_3
\\
a^{[1]}_4
\end{bmatrix}=\sigma{(z^{[1]})}=
\begin{bmatrix}
\sigma(z^{[1]}_1)
\\
\sigma(z^{[1]}_2)
\\
\sigma(z^{[1]}_3)
\\
\sigma(z^{[1]}_4)
\end{bmatrix}=
\begin{bmatrix}
\sigma({w^{[1]}_1}^T x^{(1)}+b^{[1]}_1)
\\
\sigma({w^{[1]}_2}^T x^{(1)}+b^{[1]}_2)
\\
\sigma({w^{[1]}_3}^T x^{(1)}+b^{[1]}_3)
\\
\sigma({w^{[1]}_4}^T x^{(1)}+b^{[1]}_4)
\end{bmatrix},\
a^{[1]}\in \mathbb{R}^{4 \times 1}
\]</div>
<p><strong>3、输出层</strong>：再次使用激活函数生成一个实数值，并传递给 <span class="arithmatex">\(\hat{y}\)</span></p>
<ul>
<li>也有两个相关参数：权重 <span class="arithmatex">\(w^{[2]}\in \mathbb{R}^{1\times 4}\)</span> 和偏差 <span class="arithmatex">\(b^{[2]}\in \mathbb{R}^{1\times 1}\)</span></li>
</ul>
<div class="arithmatex">\[
a^{[2]}=\sigma({z^{[2]}})=\sigma({w^{[2]}}a^{[1]}+b^{[2]})=\hat{y},\ a^{[2]}\in \mathbb{R}
\]</div>
<h4 id="312">3.1.2 网络输出原理<a class="headerlink" href="#312" title="Permanent link">&para;</a></h4>
<p><strong>1、逻辑回归神经网络</strong></p>
<ul>
<li>逻辑回归神经网络实际上就是，在每个节点上，先计算逻辑回归，然后将结果进行计算 sigmoid，计算多次后的结果即为神经网络的输出结果。</li>
</ul>
<p><strong>2 向量化实现神经网络</strong></p>
<ul>
<li>在实现神经网络时，最好使用向量化计算，而在进行向量化时，当 一层中有不同的神经元（多个节点）时，可以把他们堆叠起来，组成一个向量。</li>
<li>通过把训练样本堆叠在矩阵的不同列中，使得神经网络不仅能计算单个样本上的输出值，也能计算整个训练样本集上的输出值。 </li>
</ul>
<h3 id="32">3.2 向量化样本<a class="headerlink" href="#32" title="Permanent link">&para;</a></h3>
<h4 id="321">3.2.1 单样本向量化<a class="headerlink" href="#321" title="Permanent link">&para;</a></h4>
<p>1、通过向量化运算，得出隐藏层 <span class="arithmatex">\(a^{[1]}\)</span> 里四个逻辑回归的输出</p>
<div class="arithmatex">\[
\begin{align*}
z^{[1]}&amp;=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}\\\\
a^{[1]}&amp;=\sigma(z^{[1]})
\end{align*}
\]</div>
<p>2、用 <span class="arithmatex">\(a^{[1]}\)</span> 的四个输出作为 <span class="arithmatex">\(a^{[2]}\)</span> 层的输入</p>
<div class="arithmatex">\[
\begin{align*}
z^{[2]}&amp;=W^{[1]}a^{[0]}+b^{[1]}\\\\
a^{[2]}&amp;=\sigma(z^{[2]})=\hat{y}
\end{align*}
\]</div>
<h4 id="322">3.2.2 多样本向量化<a class="headerlink" href="#322" title="Permanent link">&para;</a></h4>
<p>基于多个训练样本向量化神经网络方法，应该避免使用 for 循环，将样本们及其相关参数的向量水平方向堆叠成一个矩阵，对于每个矩阵的元素：</p>
<ul>
<li>从左到右表示每一个样本，<span class="arithmatex">\(m\)</span> 个样本</li>
<li>从上到下表示每个样本中的不同节点，<span class="arithmatex">\(n_x\)</span> 个节点</li>
</ul>
<h4 id="323">3.2.3 前向传播向量化步骤<a class="headerlink" href="#323" title="Permanent link">&para;</a></h4>
<p><strong>1、输入层</strong></p>
<div class="arithmatex">\[
X=\begin{bmatrix}
  |&amp;  |&amp;  |&amp;  |\\
  x^{(1)}&amp;  x^{(2)}&amp;  \cdots&amp;  x^{(m)}\\
  |&amp;  |&amp;  |&amp;  |
\end{bmatrix},\ X\in \mathbb{R}^{(n_x,\ m)}
\]</div>
<p><strong>2、第一层（隐藏层）</strong></p>
<div class="arithmatex">\[
\begin{align*}
Z^{[1]}&amp;=W^{[1]}X+b^{[1]}=\begin{bmatrix}
  |&amp;  |&amp;  |&amp;  |\\
  Z^{[1](1)}&amp;  Z^{[1](2)}&amp;  \cdots&amp;  Z^{[1](m)}\\
  |&amp;  |&amp;  |&amp;  |
\end{bmatrix},\ Z^{[1]}\in \mathbb{R}^{(n_x,\ m)}\\\\
A^{[1]}&amp;=\sigma(Z^{[1]})=\begin{bmatrix}
  |&amp;  |&amp;  |&amp;  |\\
  a^{[1](1)}&amp;  a^{[1](2)}&amp;  \cdots&amp;  a^{[1](m)}\\
  |&amp;  |&amp;  |&amp;  |
\end{bmatrix},\ A^{[1]}\in \mathbb{R}^{(n_x,\ m)}
\end{align*}
\]</div>
<p><strong>3、第二层（输出层）</strong></p>
<div class="arithmatex">\[
\begin{align*}
Z^{[2]}&amp;=W^{[2]}A^{[1]}+b^{[2]}=\begin{bmatrix}
  |&amp;  |&amp;  |&amp;  |\\
  Z^{[2](1)}&amp;  Z^{[2](2)}&amp;  \cdots&amp;  Z^{[2](m)}\\
  |&amp;  |&amp;  |&amp;  |
\end{bmatrix},\ Z^{[2]}\in \mathbb{R}^{(n_x,\ m)}\\\\
A^{[2]}&amp;=\sigma(Z^{[2]})=\begin{bmatrix}
  |&amp;  |&amp;  |&amp;  |\\
  a^{[2](1)}&amp;  a^{[2](2)}&amp;  \cdots&amp;  a^{[2](m)}\\
  |&amp;  |&amp;  |&amp;  |
\end{bmatrix},\ A^{[2]}\in \mathbb{R}^{(n_x,\ m)}
\end{align*}
\]</div>
<h4 id="324">3.2.4 向量化证明<a class="headerlink" href="#324" title="Permanent link">&para;</a></h4>
<p>一个神经网络中的不同层，大致上都在做着相同的事情，或者说一遍又一遍地做着相同的计算。</p>
<p><a class="glightbox" href="../img/image-20230802180345166.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230802180345166" src="../img/image-20230802180345166.png" /></a></p>
<p><center>图 3.2 多样本向量化示例图</center></p>
<h3 id="33">3.3 激活函数<a class="headerlink" href="#33" title="Permanent link">&para;</a></h3>
<h4 id="331">3.3.1 为什么使用非线性激活函数？<a class="headerlink" href="#331" title="Permanent link">&para;</a></h4>
<p><strong>1、线性激活函数</strong></p>
<ul>
<li>恒等激活函数，使用后结果只是输入数据的线性变换，只有在研究 <strong>回归问题</strong> 时才具有实际意义</li>
<li>在输出层可以使用线性激活函数，其他层几乎不会使用</li>
</ul>
<p><strong>2、非线性激活函数</strong></p>
<p>很多实际问题都是非线性的，为了加入非线性因素，提高神经网络对实际问题的表达能力，需要对数据进行处理，让计算机根据自定义标准对数据进行判断，常用的有：</p>
<ul>
<li>sigmoid 函数、<span class="arithmatex">\(\tanh(z)\)</span> 函数、ReLU 函数等</li>
</ul>
<h4 id="332-sigmoid">3.3.2 sigmoid 函数及其导数<a class="headerlink" href="#332-sigmoid" title="Permanent link">&para;</a></h4>
<p><strong>1、sigmoid 函数</strong></p>
<p>在前文中使用的激活函数都是 <span class="arithmatex">\(\sigma(z)\)</span> 函数，进行二元分类时，输出层使用 <span class="arithmatex">\(\sigma(z)\)</span> 函数的效果会比较好。其他的分类最好不要使用。</p>
<div class="arithmatex">\[
a=g(z)=\sigma(z)=\frac{1}{1+e^{-z}},\ a\in [0,1]
\]</div>
<p><strong>2、sigma 函数在 z 点上的导数（斜率）</strong></p>
<div class="arithmatex">\[
\begin{align*}
g'(z)=\frac{d}{dz}g(z)&amp;=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})\\\\
&amp;=g(z)[1-g(z)]\\\\
&amp;=a(1-a)
\end{align*}
\]</div>
<p><a class="glightbox" href="../img/image-20230802203836947.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230802203836947" src="../img/image-20230802203836947.png" /></a></p>
<p><center>图 3.3 sigma 函数图像</center></p>
<h4 id="333-tanhz">3.3.3 <span class="arithmatex">\(\tanh(z)\)</span> 函数及其导数<a class="headerlink" href="#333-tanhz" title="Permanent link">&para;</a></h4>
<p><strong>1、<span class="arithmatex">\(\tanh(z)\)</span> 函数</strong></p>
<p><span class="arithmatex">\(\tanh(z)\)</span> 函数是 <span class="arithmatex">\(\sigma()\)</span> 函数平移后的版本。它的平均值更接近 0，使得计算出数据具有更好的 <strong>中心性</strong>，计算效果会更好。</p>
<div class="arithmatex">\[
a=g(z)=\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}},\ a\in [-1,1]
\]</div>
<p><strong>2、<span class="arithmatex">\(\tanh(z)\)</span> 函数在 z 点上的导数（斜率）</strong></p>
<div class="arithmatex">\[
\begin{align*}
g'(z)=\frac{d}{dz}g(z)&amp;=1-[\tanh(z)]^2\\\\
&amp;=1-g(z)^2\\\\
&amp;=1-a^2
\end{align*}
\]</div>
<p><a class="glightbox" href="../img/image-20230802204050789.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230802204050789" src="../img/image-20230802204050789.png" /></a></p>
<p><center>图 3.4 tanh 函数图像</center></p>
<h4 id="334-relu">3.3.4 ReLU 函数及其导数<a class="headerlink" href="#334-relu" title="Permanent link">&para;</a></h4>
<p><strong>1、ReLU 函数</strong></p>
<p>ReLU 函数，作为激活函数适用性相当广泛，是激活函数的 <strong>默认选择</strong>，相比其他激活函数计算速度更快。</p>
<div class="arithmatex">\[
a=max(0,\ z)
\]</div>
<p><strong>2、ReLU 函数在 z 点上的导数（斜率）</strong></p>
<ul>
<li><span class="arithmatex">\(z&gt;0\)</span>，斜率为正（导数为 1）</li>
<li>对于 <span class="arithmatex">\(z=0\)</span> 时，导数的值无关紧要，可以是 0，也可以是 1</li>
<li><span class="arithmatex">\(z&lt;0\)</span>，斜率为负</li>
</ul>
<div class="arithmatex">\[
\begin{align*}
g'(z)=\frac{d}{dz}g(z)&amp;=
\left\{\begin{matrix}
0 &amp; ,if\ z&lt;0 \\
1 &amp; ,if\ z&gt;0 \\
undefine &amp; ,if\ z=0
\end{matrix}\right.
\end{align*}
\]</div>
<p><a class="glightbox" href="../img/image-20230802204136339.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230802204136339" src="../img/image-20230802204136339.png" /></a></p>
<p><center>图 3.5 ReLU 函数图像</center></p>
<h4 id="335-leaky-relu">3.3.5 Leaky ReLU 函数及其导数<a class="headerlink" href="#335-leaky-relu" title="Permanent link">&para;</a></h4>
<p><strong>1、Leaky ReLU</strong></p>
<p>由于 ReLU 函数在小于 0 时，导数为 0，所以在小于 0 时，对 ReLU 函数进行优化。</p>
<div class="arithmatex">\[
a=max(0.01z,\ z)
\]</div>
<p><strong>2、Leaky ReLU 函数在 z 点上的导数（斜率）</strong></p>
<div class="arithmatex">\[
\begin{align*}
g'(z)=\frac{d}{dz}g(z)&amp;=
\left\{\begin{matrix}
0.01 &amp; ,if\ z&lt;0 \\
1 &amp; ,if\ z&gt;0 \\
undefine &amp; ,if\ z=0
\end{matrix}\right.
\end{align*}
\]</div>
<p><a class="glightbox" href="../img/image-20230802204204632.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230802204204632" src="../img/image-20230802204204632.png" /></a></p>
<p><center>图 3.6 Leaky ReLU 函数图像</center></p>
<h4 id="336">3.3.6 激活函数的选择<a class="headerlink" href="#336" title="Permanent link">&para;</a></h4>
<p><strong>1、一般情况下：</strong>所有层全部使用 ReLU 函数</p>
<p><strong>2、二元分类时：</strong>在隐藏层使用 ReLU 函数，而在输出层使用 <span class="arithmatex">\(\sigma(z)\)</span> 函数（不同层的激活函数可以不同）</p>
<p><strong>3、不确定时：</strong>在交叉验证数据集中对不同激活函数进行测试，看哪一个的参数效果更好，就使用哪一个</p>
<h3 id="34-">3.4 后向传播 - 梯度下降<a class="headerlink" href="#34-" title="Permanent link">&para;</a></h3>
<p>正向传播：求出预测值</p>
<p>反向传播：求出梯度（斜率）</p>
<h4 id="341">3.4.1 单个样本的梯度计算<a class="headerlink" href="#341" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
\begin{align*}
&amp;dz^{[2]}=a^{[2]}-y\\\\
&amp;dW^{[2]}=dz^{[2]}{a^{[1]}}^T\\\\
&amp;db^{[2]}=dz^{[2]}\\\\
&amp;dz^{[1]}={W^{[2]}}^Tdz^{[2]}*g'^{[1]}(z^{[1]})\\\\
&amp;dW^{[1]}=dz^{[1]}x^T\\\\
&amp;db^{[1]}=dz^{[1]}
\end{align*}
\]</div>
<h4 id="342">3.4.2 多样本梯度计算<a class="headerlink" href="#342" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
\begin{align*}
&amp;dZ^{[2]}=A^{[2]}-Y\\\\
&amp;dW^{[2]}=\frac{1}{m}dZ^{[2]}{A^{[1]}}^T\\\\
&amp;db^{[2]}=\frac{1}{m}np.sum(dz^{[2]},\ axis=1,\ keepdims=True)\\\\
&amp;dZ^{[1]}={W^{[2]}}^TdZ^{[2]}*g'^{[1]}(Z^{[1]})\\\\
&amp;dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T\\\\
&amp;db^{[1]}=\frac{1}{m}np.sum(dz^{[1]},\ axis=1,\ keepdims=True)
\end{align*}
\]</div>
<h4 id="343">3.4.3 学习率控制梯度下降<a class="headerlink" href="#343" title="Permanent link">&para;</a></h4>
<div class="arithmatex">\[
\begin{align*}
W&amp;=W-\alpha dW^{[1]}\\\\
b&amp;=b-\alpha db^{[1]}
\end{align*}
\]</div>
<h3 id="35">3.5 随机初始化权重<a class="headerlink" href="#35" title="Permanent link">&para;</a></h3>
<p>在逻辑回归的问题中，把权重参数初始化为零是可行的。但把神经网络的权重参数全部初始化为零，并使用梯度下降，将无法获得预期的效果，所以需要对权重进行随机初始化。</p>
<div class="arithmatex">\[
\begin{align*}
W^{[1]}&amp;=np.random.randn((m,n))*0.01\\\\
b^{[1]}&amp;=np.zero((m,n))
\end{align*}
\]</div>
<p><strong>1、随机初始化权重</strong>：将权重 W 初始化为 <span class="arithmatex">\((m,n)\)</span> 的高斯分布随机变量</p>
<ul>
<li>随机值最好非常非常小，因为小的权重更有利于计算激活函数。</li>
<li>如果权重过大，会导致激活函数（如 sigmoid 函数和 <span class="arithmatex">\(\tanh\)</span> 函数）过于饱和，梯度接近于 0，不利于进行梯度下降，会减慢学习速度.</li>
<li>对于较为深层的神经网络，应该设置更为小的参数</li>
</ul>
<p><strong>2、将偏差初始化为 0</strong>：偏差 <span class="arithmatex">\(b\)</span> 仍然可以是 0</p>
<h2 id="4">4 深层神经网络<a class="headerlink" href="#4" title="Permanent link">&para;</a></h2>
<h3 id="41">4.1 基本概念<a class="headerlink" href="#41" title="Permanent link">&para;</a></h3>
<p>无法预先准确地判断需要多深的神经网络，可以先尝试着使用 logistic 回归（单层神经网路），然后再逐步增加一到两个隐藏层，把隐藏层数量当作一个可以自由调整数值大小的超参数去调试，找到比较适合的深度。</p>
<h3 id="42">4.2 确保维度正确<a class="headerlink" href="#42" title="Permanent link">&para;</a></h3>
<p>在实现深度神经网络时，清晰地知道每个矩阵和向量的维度，能够很好地排除编程中的 bug。</p>
<p><a class="glightbox" href="../img/image-20230804142613142.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230804142613142" src="../img/image-20230804142613142.png" /></a></p>
<p><center>图 4.1 深层神经网络结构图</center></p>
<p>1、对于网络的第一层隐藏层：</p>
<div class="arithmatex">\[
\begin{align*}
z^{[1]}&amp;=w^{[1]}\cdot x+b^{[1]}\\\\\
a^{[1]}&amp;=g^{[1]}(z^{[1]})
\end{align*}
\]</div>
<ul>
<li><span class="arithmatex">\(w^{[1]}\in \mathbb{R}^{3\times 2}\)</span>，维度为 <span class="arithmatex">\((n^{[1]},\ n^{[0]})\)</span></li>
<li><span class="arithmatex">\(x\in \mathbb{R}^{2\times 1}\)</span>，维度为 <span class="arithmatex">\((n^{[0]},\ 1)\)</span></li>
<li><span class="arithmatex">\(b^{[1]},\ z^{[1]},\ a^{[1]} \in \mathbb{R}^{3\times 1}\)</span>，维度都是 <span class="arithmatex">\((n^{[1]},\ 1)\)</span></li>
</ul>
<p>2、对于有 L 层的神经网络，</p>
<ul>
<li>权重参数 <span class="arithmatex">\(w^{[L]}\)</span> 的维度必为 <span class="arithmatex">\((n^{[L]},\ n^{[L-1]})\)</span></li>
<li>
<p>偏差参数 <span class="arithmatex">\(b^{[L]}\)</span> 的维度必为 <span class="arithmatex">\((n^{[L]},\ 1)\)</span></p>
</li>
<li>
<p><span class="arithmatex">\(dw^{[L]}\)</span> 的维度必为 <span class="arithmatex">\((n^{[L]},\ n^{[L-1]})\)</span></p>
</li>
<li><span class="arithmatex">\(db^{[L]}\)</span> 的维度必为 <span class="arithmatex">\((n^{[L]},\ 1)\)</span></li>
</ul>
<h3 id="43">4.3 构建神经网络结构<a class="headerlink" href="#43" title="Permanent link">&para;</a></h3>
<h4 id="431">4.3.1 正向传播<a class="headerlink" href="#431" title="Permanent link">&para;</a></h4>
<p>对于第 L 层的某个节点</p>
<p><strong>1、输入：</strong> <span class="arithmatex">\(a^{[L-1]}\)</span></p>
<p><strong>2、输出：</strong></p>
<ul>
<li><span class="arithmatex">\(a^{[L]}\)</span>，作为下一层的输入</li>
<li>缓存 <span class="arithmatex">\(Z^{[L]},\ W^{[L]},\ b^{[L]}\)</span> 的值，用于反向传播时计算梯度</li>
</ul>
<h4 id="432">4.3.2 反向传播<a class="headerlink" href="#432" title="Permanent link">&para;</a></h4>
<p><strong>1、输入：</strong><span class="arithmatex">\(da^{[L]}\)</span></p>
<p><strong>2、输出：</strong><span class="arithmatex">\(da^{[L-1]},\ dW^{[L]},\ db^{[L]}\)</span></p>
<ul>
<li>通过缓存的数值，进行梯度计算</li>
</ul>
<p>3、梯度下降：利用计算出的导数不断减小权重和偏差值</p>
<p><a class="glightbox" href="../img/image-20230804154112954.png" data-type="image" data-width="100%" data-height="auto" data-desc-position="bottom"><img alt="image-20230804154112954" src="../img/image-20230804154112954.png" /></a></p>
<p><center>图 4.2 梯度下降流程图</center></p>
<h3 id="44">4.4 超参数调优<a class="headerlink" href="#44" title="Permanent link">&para;</a></h3>
<p>超参数是指能够控制参数的参数。比如：</p>
<ul>
<li>学习率：<span class="arithmatex">\(\alpha\)</span></li>
<li>每个隐藏层的单元数</li>
<li>神经网络总层数</li>
<li>隐藏层的迭代次数</li>
</ul>

  

<!-- taken from 
https://github.com/squidfunk/mkdocs-material/blob/master/src/partials/source-file.html -->

<hr />
<div class="md-source-file">
    <small>

        <!-- mkdocs-git-revision-date-localized-plugin -->
        
        最后更新:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class="timeago" datetime="2023-08-16T11:59:28+08:00" locale="en"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2023-08-16</span>
        
        <br />
        创建日期:
        <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class="timeago" datetime="2023-08-16T10:20:16+08:00" locale="en"></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2023-08-16</span>
        

        <!-- mkdocs-git-revision-date-plugin -->
        

        <br />
        
        作者:
        <span class='git-page-authors git-authors'><a href='mailto:1612990205@qq.com'>gis-xh</a></span>
        
    </small>
</div>





<!-- Insert Giscus generated snippet here -->
<!-- 插入 Giscus 生成的代码块 -->
<script src="https://giscus.app/client.js" data-repo="gis-xh/mkdocs-site" data-repo-id="R_kgDOJqGKjQ"
    data-category="Announcements" data-category-id="DIC_kwDOJqGKjc4CW4Tq" data-mapping="pathname" data-strict="0"
    data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme"
    data-lang="zh-CN" crossorigin="anonymous" async>
    </script>

<!-- 下面保持不动即可 -->
<!-- Synchronize Giscus theme with palette -->
<script>
    var giscus = document.querySelector("script[src*=giscus]")

    /* Set palette on initial load */
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate" ? "dark" : "light"
        giscus.setAttribute("data-theme", theme)
    }

    /* Register event handlers after documented loaded */
    document.addEventListener("DOMContentLoaded", function () {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function () {
            var palette = __md_get("__palette")
            if (palette && typeof palette.color === "object") {
                var theme = palette.color.scheme === "slate" ? "dark" : "light"

                /* Instruct Giscus to change theme */
                var frame = document.querySelector(".giscus-frame")
                frame.contentWindow.postMessage(
                    { giscus: { setConfig: { theme } } },
                    "https://giscus.app"
                )
            }
        })
    })
</script>

                


              </article>
            </div>
          
          
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            回到页面顶部
          </button>
        
      </main>
      
        <footer class="md-footer">
  
    
      
      <nav class="md-footer__inner md-grid" aria-label="页脚" >
        
          
          <a href="../0102/" class="md-footer__link md-footer__link--prev" aria-label="上一页: 0102" rel="prev">
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
            </div>
            <div class="md-footer__title">
              <span class="md-footer__direction">
                上一页
              </span>
              <div class="md-ellipsis">
                0102
              </div>
            </div>
          </a>
        
        
          
          <a href="../02/" class="md-footer__link md-footer__link--next" aria-label="下一页: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization" rel="next">
            <div class="md-footer__title">
              <span class="md-footer__direction">
                下一页
              </span>
              <div class="md-ellipsis">
                Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
              </div>
            </div>
            <div class="md-footer__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
            </div>
          </a>
        
      </nav>
    
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2023 gis-xh
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
    <a href="https://github.com/gis-xh" target="_blank" rel="noopener" title="GitHub | gis-xh" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": ["content.code.copy", "navigation.tabs", "navigation.top", "navigation.footer", "navigation.indexes", "navigation.expand", "search.suggest", "search.highlight", "search.share"], "search": "../../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
        
          <script src="../../../js/timeago.min.js"></script>
        
      
        
          <script src="../../../js/timeago_mkdocs_material.js"></script>
        
      
        
          <script src="../../../mkdocs/js/katex.js"></script>
        
      
        
          <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/katex.min.js"></script>
        
      
        
          <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.7/contrib/auto-render.min.js"></script>
        
      
    
  <script>document$.subscribe(() => {const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});})</script></body>
</html>