{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Numpy 基础（可选作业）\n",
    "\n",
    "欢迎来到你的第一个作业。这次练习向你简要地介绍 Python。即使你之前使用过 Python，这也有助于你熟悉我们将使用的函数。\n",
    "\n",
    "**Instructions:**\n",
    "- 你将使用 Python3。\n",
    "- 避免使用循环 (for/while)，除非你被明确告知需要使用。\n",
    "- 不要修改单元格中的一些注释。如果你改变这一点，你的作业就不会被评分。每个包含该注释的单元格应该只包含一个函数。\n",
    "- 编写函数后，运行下面的单元格，检查结果是否正确。\n",
    "\n",
    "**完成这次作业后，你将学会：**\n",
    "- 能够使用 iPython Notebooks\n",
    "- 能够使用 numpy 函数和 numpy 矩阵/向量 操作\n",
    "- 理解 \"broadcasting\" 的概念\n",
    "- 能够向量化代码\n",
    "\n",
    "让我们直接开始吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关于 iPython Notebooks ##\n",
    "\n",
    "iPython notebook 是嵌入在网页中的交互式编码环境。在本章中，我们将使用 iPython notebooks。你只需要在 ### START code HERE ### 和 ### END code HERE ### 注释之间编写代码。编写代码后，你可以通过按下 SHIFT+ENTER 或点击笔记本右上角的 run cell（用 play 符号表示）来运行单元格。\n",
    "\n",
    "我们通常会在注释中指定 \"(≈ X lines of code)\"，告诉你需要编写多少代码。这只是一个粗略的估计，所以如果你的代码更长或更短，不要感到沮丧。\n",
    "\n",
    "**练习**: 将 test 设置为 `\"Hello World\"` 在下面的单元格中打印 “Hello World” 并运行下面的两个单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 1 line of code)\n",
    "test = 'Hello World'\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: Hello World\n"
     ]
    }
   ],
   "source": [
    "print (\"test: \" + test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "test: Hello World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**你需要记住：**\n",
    "- 使用 SHIFT+ENTER 运行你的单元格\n",
    "- 仅使用 Python3 在指定区域编写代码\n",
    "- 不修改代码以外的指定区域"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 使用 numpy 创建基础函数 ##\n",
    "\n",
    "Numpy 是 Python 中用于科学计算的主要包。它由一个大型社区 (www.numpy.org) 维护。在本练习中，你将学习几个关键的 numpy 函数，例如 `np.exp`、`np.log` 和 `np.shape`。你需要知道如何在以后的作业中使用这些函数。\n",
    "\n",
    "### 1.1 - sigmoid 函数, `np.exp()` ###\n",
    "\n",
    "使用 `np.exp()` 前，你将使用 `math.exp()` 去实现 sigmoid 函数。然后你将看到为什么 `np.exp()` 是优于 `math.exp()` 的。\n",
    "\n",
    "**练习**: 创建一个返回实数 $x$ 的 sigmoid 值的函数。对指数函数使用 `math.exp(x)`。\n",
    "\n",
    "**提示**:\n",
    "$sigmoid(x) = \\frac{1}{1+e^{-x}}$ 有时也被称为逻辑函数。它是一个不仅用于机器学习（逻辑回归）也用于深度学习的非线性函数。\n",
    "\n",
    "<img src=\"images/Sigmoid.png\" style=\"width:500px;height:228px;\">\n",
    "\n",
    "要引用属于特定包的函数，可以使用 `package_name.function()` 调用它。运行下面的代码来看看一个使用 `math.exp()` 的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: basic_sigmoid\n",
    "\n",
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    计算 x 的 sigmoid。\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + math.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9525741268224334"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_sigmoid(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**: \n",
    "<table style = \"width:40%\">\n",
    "    <tr>\n",
    "        <td>**basic_sigmoid(3)**</td> \n",
    "        <td>0.9525741268224334 </td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际上，我们在深度学习中很少使用 \"math\" 库，因为函数输入的是真实值。在深度学习中我们主要使用矩阵和向量。这也是 numpy 更有用的原因。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "bad operand type for unary -: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m### 我们在深度学习中使用 \"numpy\" 而不是 \"math\" 的原因 ###\u001b[39;00m\n\u001b[0;32m      2\u001b[0m x \u001b[39m=\u001b[39m [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m basic_sigmoid(x)\n",
      "Cell \u001b[1;32mIn[3], line 17\u001b[0m, in \u001b[0;36mbasic_sigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[39m计算 x 的 sigmoid。\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39ms -- sigmoid(x)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m### START CODE HERE ### (≈ 1 line of code)\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m math\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39;49mx))\n\u001b[0;32m     18\u001b[0m \u001b[39m### END CODE HERE ###\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m s\n",
      "\u001b[1;31mTypeError\u001b[0m: bad operand type for unary -: 'list'"
     ]
    }
   ],
   "source": [
    "### 我们在深度学习中使用 \"numpy\" 而不是 \"math\" 的原因 ###\n",
    "x = [1, 2, 3]\n",
    "basic_sigmoid(x) # 当你运行时，你会看到一个错误，因为 x 是一个向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事实上，如果 $ x = (x_1, x_2, ..., x_n)$ 是一个行向量，那么 $np.exp(x)$ 将会计算 x 中每个元素的指数函数。这个输出将会是：$np.exp(x) = (e^{x_1}, e^{x_2}, ..., e^{x_n})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.71828183  7.3890561  20.08553692]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# example of np.exp\n",
    "x = np.array([1, 2, 3])\n",
    "print(np.exp(x)) # result is (exp(1), exp(2), exp(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "而且，如果 x 是一个向量，则 Python 操作如如 $s = x + 3$ 或 $s = \\frac{1}{x}$ 将输出 s 作为与 x 相同大小的向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# 向量运算示例\n",
    "x = np.array([1, 2, 3])\n",
    "print (x + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有时你需要 numpy 函数的更多信息，我们鼓励你去访问 [the official documentation](https://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.exp.html). \n",
    "\n",
    "你也可以创建一个新的单元格在 notebook 并写上 `np.exp?` 以快速访问说明文档。\n",
    "\n",
    "**练习**: 使用 numpy 执行 sigmoid 函数。 \n",
    "\n",
    "**说明**: x 可以是一个实数，一个向量，或是一个矩阵。在 Numpy 中我们用来表示这些形状（向量、矩阵……）的数据结构被称为 numpy 数组。 \n",
    "\n",
    "$$\n",
    "\\text{For } x \\in \\mathbb{R}^n \\text{,     } sigmoid(x) = sigmoid\\begin{pmatrix}\n",
    "    x_1  \\\\\\\\\n",
    "    x_2  \\\\\\\\\n",
    "    ...  \\\\\\\\\n",
    "    x_n  \\\\\\\\\n",
    "\\end{pmatrix} = \\begin{pmatrix}\n",
    "    \\frac{1}{1+e^{-x_1}}  \\\\\\\\\n",
    "    \\frac{1}{1+e^{-x_2}}  \\\\\\\\\n",
    "    ...  \\\\\\\\\n",
    "    \\frac{1}{1+e^{-x_n}}  \\\\\\\\\n",
    "\\end{pmatrix}\\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid\n",
    "\n",
    "import numpy as np # 这意味着你可以通过np.function()而不是numpy.function()来访问numpy中的函数。\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    计算 x 的 sigmoid。\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.73105858, 0.88079708, 0.95257413])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**: \n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid([1,2,3])**</td> \n",
    "        <td> array([ 0.73105858,  0.88079708,  0.95257413]) </td> \n",
    "    </tr>\n",
    "</table> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Sigmoid 梯度\n",
    "\n",
    "正如你在课程中看到的，你需要计算梯度使用反向传播去优化损失函数。让我们开始编写你的第一个梯度函数。\n",
    "\n",
    "**练习**: 执行 sigmoid_grad() 函数去计算 sigmoid 函数的关于输入值 x 的梯度。公式为: \n",
    "\n",
    "$$\n",
    "sigmoid\\_derivative(x) = \\sigma'(x) = \\sigma(x) (1 - \\sigma(x))\\tag{2}\n",
    "$$\n",
    "\n",
    "这个函数通常分两步编写：\n",
    "1. 将 s 设置为 x 的 sigmoid 值。你可能会发现 sigmoid(x) 函数很有用。\n",
    "2. 计算 $\\sigma'(x) = s(1-s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: sigmoid_derivative\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    计算 sigmoid 函数的关于输入值 x 的梯度（也被称为斜率或导数）。\n",
    "    你可以把 sigmoid 函数的输出存储到变量中，然后用它来计算梯度。\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array\n",
    "\n",
    "    Return:\n",
    "    ds -- Your computed gradient.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    ds = s * (1 - s)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print (\"sigmoid_derivative(x) = \" + str(sigmoid_derivative(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**: \n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr> \n",
    "        <td> **sigmoid_derivative([1,2,3])**</td> \n",
    "        <td> [ 0.19661193  0.10499359  0.04517666] </td> \n",
    "    </tr>\n",
    "</table> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - 重塑数组 ###\n",
    "\n",
    "两个在深度学习中常用的 numpy 函数是 [np.shape](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html) 和 [np.reshape()](https://docs.scipy.org/doc/numpy/reference/generated/numpy.reshape.html). \n",
    "- `X.shape` 被用于获取矩阵/向量 X 的形状（维度）。 \n",
    "- `X.reshape(...)` 被用于重塑 X 到一些其他的维度。 \n",
    "\n",
    "例如，在科学计算中，一张图像的形状由三维数组来表示 $(length, height, depth = 3)$。然而，当你读取一张图像作为一个算法输入时，你会将它转化为一个向量的形状 $(length*height*3, 1)$。换句话说，你可以将3D数组 “展开” 或重塑为一维向量。\n",
    "\n",
    "<img src=\"images/image2vector_kiank.png\" style=\"width:500px;height:300;\">\n",
    "\n",
    "**练习**: 执行 `image2vector()`，它接受一个形状为 $(length, height, 3)$ 的输入，并返回一个形状为 $(length*height*3, 1)$ 的向量。 例如，如果你想将一个形状为 $(a, b, c)$ 的数组 $v$ 重塑为一个形状为 $(a*b,c)$ 的向量，你可以这样做：\n",
    "\n",
    "``` python\n",
    "v = v.reshape((v.shape[0]*v.shape[1], v.shape[2])) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c\n",
    "```\n",
    "\n",
    "- 请不要将图像的维度硬编码为常量（即直接输入常数）。而是使用 `image.shape[0]`, …… 查询所需的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: image2vector\n",
    "def image2vector(image):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    image -- a numpy array of shape (length, height, depth)\n",
    "    \n",
    "    Returns:\n",
    "    v -- a vector of shape (length*height*depth, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image2vector(image) = \n",
      "[[0.67826139]\n",
      " [0.29380381]\n",
      " [0.90714982]\n",
      " [0.52835647]\n",
      " [0.4215251 ]\n",
      " [0.45017551]\n",
      " [0.92814219]\n",
      " [0.96677647]\n",
      " [0.85304703]\n",
      " [0.52351845]\n",
      " [0.19981397]\n",
      " [0.27417313]\n",
      " [0.60659855]\n",
      " [0.00533165]\n",
      " [0.10820313]\n",
      " [0.49978937]\n",
      " [0.34144279]\n",
      " [0.94630077]]\n"
     ]
    }
   ],
   "source": [
    "# 这是一个 3 × 3 × 2 的数组，通常图像将是 (num_px_x, num_px_y, 3)，其中 3 表示RGB值\n",
    "image = np.array([[[ 0.67826139,  0.29380381],\n",
    "        [ 0.90714982,  0.52835647],\n",
    "        [ 0.4215251 ,  0.45017551]],\n",
    "\n",
    "       [[ 0.92814219,  0.96677647],\n",
    "        [ 0.85304703,  0.52351845],\n",
    "        [ 0.19981397,  0.27417313]],\n",
    "\n",
    "       [[ 0.60659855,  0.00533165],\n",
    "        [ 0.10820313,  0.49978937],\n",
    "        [ 0.34144279,  0.94630077]]])\n",
    "\n",
    "print (\"image2vector(image) = \\n\" + str(image2vector(image)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**: \n",
    "\n",
    "<table style=\"width:100%\">\n",
    "     <tr> \n",
    "       <td> **image2vector(image)** </td> \n",
    "       <td> [[ 0.67826139]\n",
    " [ 0.29380381]\n",
    " [ 0.90714982]\n",
    " [ 0.52835647]\n",
    " [ 0.4215251 ]\n",
    " [ 0.45017551]\n",
    " [ 0.92814219]\n",
    " [ 0.96677647]\n",
    " [ 0.85304703]\n",
    " [ 0.52351845]\n",
    " [ 0.19981397]\n",
    " [ 0.27417313]\n",
    " [ 0.60659855]\n",
    " [ 0.00533165]\n",
    " [ 0.10820313]\n",
    " [ 0.49978937]\n",
    " [ 0.34144279]\n",
    " [ 0.94630077]]</td> \n",
    "     </tr>\n",
    "    \n",
    "   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - 规范行\n",
    "\n",
    "我们在机器学习和深度学习中使用的另一种常见技术是**规范化数据**。由于梯度下降在归一化后收敛速度更快，因此通常可以获得更好的性能。在这里通过规范化，意味着我们将 x 更改为 $ \\frac{x}{\\| x\\|} $ （将 x 的每个行向量除以它的模长 ||x||）。\n",
    "\n",
    "例如，如果\n",
    "\n",
    "$$\n",
    "x = \n",
    "\\begin{bmatrix}\n",
    "    0 & 3 & 4 \\\\\n",
    "    2 & 6 & 4\n",
    "\\end{bmatrix}\\tag{3}\n",
    "$$\n",
    "\n",
    "然后\n",
    "\n",
    "$$\\| x\\| = np.linalg.norm(x,\\ axis = 1,\\ keepdims = True) =\n",
    "\\begin{bmatrix}\n",
    "    5 \\\\\\\\\n",
    "    \\sqrt{56}\n",
    "\\end{bmatrix}\\tag{4}\n",
    "$$\n",
    "\n",
    "并且\n",
    "\n",
    "$$\n",
    "x\\_normalized = \\frac{x}{\\| x\\|} =\n",
    "\\begin{bmatrix}\n",
    "    0 & \\frac{3}{5} & \\frac{4}{5} \\\\\\\\\n",
    "    \\frac{2}{\\sqrt{56}} & \\frac{6}{\\sqrt{56}} & \\frac{4}{\\sqrt{56}}\n",
    "\\end{bmatrix}\\tag{5}\n",
    "$$\n",
    "\n",
    "请注意，你可以对不同大小的矩阵进行除法，并且效果很好：这称为广播 (broadcast)，我们将在 1.5 节介绍它。\n",
    "\n",
    "**练习**: 执行 `normalizeRows()` 将矩阵的行归一化。将此函数应用于输入矩阵 x 后，x 的每一行都应该是一个单位长度的向量（即模长为 1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: normalizeRows\n",
    "\n",
    "def normalizeRows(x):\n",
    "    \"\"\"\n",
    "    实现一个函数，对矩阵 x 的每一行进行归一化（使其具有单位长度）。\n",
    "    \n",
    "    Argument:\n",
    "    x -- 一个形状为 (n, m) 的 numpy 矩阵\n",
    "    \n",
    "    Returns:\n",
    "    x -- 按行归一化后的 numpy 矩阵，可以修改 x。\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # 计算 x_norm 作为 the norm 2 of x. 使用 np.linalg.norm(..., ord = 2, axis = ..., keepdims = True)\n",
    "    x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "    # 用 x 除以它的模长。\n",
    "    x = x / x_norm\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeRows(x) = \n",
      "[[0.         0.6        0.8       ]\n",
      " [0.13736056 0.82416338 0.54944226]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [0, 3, 4],\n",
    "    [1, 6, 4]])\n",
    "print(\"normalizeRows(x) = \\n\" + str(normalizeRows(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**: \n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr>\n",
    "          <td> **normalizeRows(x)** </td>\n",
    "          <td> [[ 0.          0.6         0.8       ]<br/>\n",
    "          [ 0.13736056  0.82416338  0.54944226]]</td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原矩阵形状: (2, 3)\n",
      "矩阵模长: (2, 1)\n"
     ]
    }
   ],
   "source": [
    "x_norm = np.linalg.norm(x, axis=1, keepdims=True)\n",
    "print(\"原矩阵形状:\", x.shape)\n",
    "print(\"矩阵模长:\", x_norm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**:\n",
    "在函数 normalizeRows() 中，你可以尝试去打印 x 和 x_norm 的形状，然后重新运行任务代码。你可能会发现它们具有不同的形状。这是正常现象，x_norm 取 x 每一行行向量的模长。所以 x_norm 的行数与 x 相同，但是只有 1 列。那么 x 除以 x_norm 是怎样进行的呢？这就是所谓的广播，我们现在就来讨论它！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Broadcasting 和 softmax 函数 ####\n",
    "numpy 中需要理解的一个非常重要的概念是 “广播”。在不同形状的数组之间执行数学运算时，它非常有用。有关广播的完整细节，您可以阅读官方文件 [broadcasting documentation](http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 使用 numpy 执行一个 softmax 函数。你可以将 softmax 视为算法需要对两个或多个类别进行分类时使用的规范化函数。您将在此专业的第二门课程中了解更多关于 softmax 的知识。\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- 对于 $ x \\in \\mathbb{R}^{1\\times n}, \\ softmax(x) = softmax(\\begin{bmatrix}\n",
    "    x_1,\\ x_2,\\ \\cdots,\\ x_n  \n",
    "\\end{bmatrix}) = \\begin{bmatrix}\n",
    "    \\frac{e^{x_1}}{\\sum_{j}e^{x_j}},\\\n",
    "    \\frac{e^{x_2}}{\\sum_{j}e^{x_j}},\\\n",
    "    \\cdots,\\\n",
    "    \\frac{e^{x_n}}{\\sum_{j}e^{x_j}} \n",
    "\\end{bmatrix} $ \n",
    "\n",
    "- 对于矩阵 $ X \\in \\mathbb{R}^{m\\times n}, \\ x_{ij} $ 是映射到矩阵 $X$ 的第 $i$ 行、第 $j$ 列的元素，因此有：\n",
    "  \n",
    "$$\n",
    "softmax(X) = softmax\\begin{bmatrix}\n",
    "    x_{11} & x_{12} & x_{13} & \\dots  & x_{1n} \\\\\\\\\n",
    "    x_{21} & x_{22} & x_{23} & \\dots  & x_{2n} \\\\\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\n",
    "    x_{m1} & x_{m2} & x_{m3} & \\dots  & x_{mn}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "    \\frac{e^{x_{11}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{12}}}{\\sum_{j}e^{x_{1j}}} & \\frac{e^{x_{13}}}{\\sum_{j}e^{x_{1j}}} & \\dots  & \\frac{e^{x_{1n}}}{\\sum_{j}e^{x_{1j}}} \\\\\\\\\n",
    "    \\frac{e^{x_{21}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{22}}}{\\sum_{j}e^{x_{2j}}} & \\frac{e^{x_{23}}}{\\sum_{j}e^{x_{2j}}} & \\dots  & \\frac{e^{x_{2n}}}{\\sum_{j}e^{x_{2j}}} \\\\\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\\\\\n",
    "    \\frac{e^{x_{m1}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m2}}}{\\sum_{j}e^{x_{mj}}} & \\frac{e^{x_{m3}}}{\\sum_{j}e^{x_{mj}}} & \\dots  & \\frac{e^{x_{mn}}}{\\sum_{j}e^{x_{mj}}}\n",
    "\\end{bmatrix} = \\begin{pmatrix}\n",
    "    softmax\\text{(first row of x)}  \\\\\\\\\n",
    "    softmax\\text{(second row of x)} \\\\\\\\\n",
    "    ...  \\\\\\\\\n",
    "    softmax\\text{(last row of x)}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: softmax\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"计算输入矩阵 X 的每一行的 softmax。\n",
    "\n",
    "    你的代码不仅适用于行向量，也适用于形状为 (n,m) 的矩阵\n",
    "\n",
    "    Argument:\n",
    "    x -- 一个形状为 (n, m) 的 numpy 矩阵\n",
    "\n",
    "    Returns:\n",
    "    s -- 一个等于 X 的 softmax, 形状为 (n,m) 的 numpy 矩阵\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    # 对 x 的每一个元素求 exp(). 使用 np.exp(...)\n",
    "    x_exp = np.exp(x)\n",
    "    # 创建一个向量 x_sum 对 x_exp 的每一行求和。使用 np.sum(..., axis = 1, keepdims = True)\n",
    "    x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "    # 通过 x_exp 除以 x_sum 计算 softmax(x). 它应该自动使用 numpy 的 broadcasting。\n",
    "    s = x_exp / x_sum\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax(x) = \n",
      "[[9.80897665e-01 8.94462891e-04 1.79657674e-02 1.21052389e-04\n",
      "  1.21052389e-04]\n",
      " [8.78679856e-01 1.18916387e-01 8.01252314e-04 8.01252314e-04\n",
      "  8.01252314e-04]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([\n",
    "    [9, 2, 5, 0, 0],\n",
    "    [7, 5, 0, 0 ,0]])\n",
    "print(\"softmax(x) = \\n\" + str(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "     <tr> \n",
    "       <td> **softmax(x)** </td> \n",
    "       <td> [[  9.80897665e-01   8.94462891e-04   1.79657674e-02   1.21052389e-04\n",
    "    1.21052389e-04]\n",
    " [  8.78679856e-01   1.18916387e-01   8.01252314e-04   8.01252314e-04\n",
    "    8.01252314e-04]]</td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**:\n",
    "- 如果你重新运行任务单元格打印 x_exp, x_sum  还有 s 的形状，你将看到 x_sum 的形状是 (2,1) 而 x_exp 和 s 的形状是 (2,5)。**x_exp/x_sum** 依赖于 python 广播而正常计算。\n",
    "\n",
    "恭喜你！现在你已经很好地理解了 python numpy，并实现了一些将在深度学习中使用的有用函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**你需要记住：**\n",
    "\n",
    "- `np.exp(x)` 适用于任何 `np.array` x 并将指数函数应用于每一个坐标元素\n",
    "- sigmoid 函数及其梯度\n",
    "- `image2vector` 经常在深度学习中使用\n",
    "- `np.reshape` 被广泛使用。接下来，你将看到保持矩阵/向量的维度一致性有助于消除很多 bug\n",
    "- numpy 具有高效的内置函数\n",
    "- broadcasting 是非常有用的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2 - 向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在深度学习中，你需要处理非常大的数据集。因此，一个不是计算最优的函数可能会成为算法的巨大瓶颈，并导致模型运行时间过长。为了确保代码的计算效率，你将使用向量化。例如，试着区分以下 dot 点积/outer 外积/ elementwise 逐个元素乘积的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量点积 = 278\n",
      " ----- 计算时间 = 0.0ms\n",
      "向量外积 = \n",
      "[[81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [63. 14. 14. 63.  0. 63. 14. 35.  0.  0. 63. 14. 35.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [81. 18. 18. 81.  0. 81. 18. 45.  0.  0. 81. 18. 45.  0.  0.]\n",
      " [18.  4.  4. 18.  0. 18.  4. 10.  0.  0. 18.  4. 10.  0.  0.]\n",
      " [45. 10. 10. 45.  0. 45. 10. 25.  0.  0. 45. 10. 25.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      " ----- 计算时间 = 0.0ms\n",
      "向量各元素积 = [81.  4. 10.  0.  0. 63. 10.  0.  0.  0. 81.  4. 25.  0.  0.]\n",
      " ----- 计算时间 = 0.0ms\n",
      "gdot = [23.13842153 22.91922857 14.70687609]\n",
      " ----- 计算时间 = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### 经典向量点积实现 ###\n",
    "tic = time.process_time()\n",
    "dot = 0\n",
    "for i in range(len(x1)):\n",
    "    dot+= x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"向量点积 = \" + str(dot) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 经典向量外积实现 ###\n",
    "tic = time.process_time()\n",
    "outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zeros\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x2)):\n",
    "        outer[i,j] = x1[i]*x2[j]\n",
    "toc = time.process_time()\n",
    "print (\"向量外积 = \\n\" + str(outer) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 经典向量元素积实现 ###\n",
    "tic = time.process_time()\n",
    "mul = np.zeros(len(x1))\n",
    "for i in range(len(x1)):\n",
    "    mul[i] = x1[i]*x2[i]\n",
    "toc = time.process_time()\n",
    "print (\"向量各元素积 = \" + str(mul) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 经典通用点积实现 ###\n",
    "W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy array\n",
    "tic = time.process_time()\n",
    "gdot = np.zeros(W.shape[0])\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(len(x1)):\n",
    "        gdot[i] += W[i,j]*x1[j]\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(gdot) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot = 278\n",
      " ----- 计算时间 = 0.0ms\n",
      "outer = \n",
      "[[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]\n",
      " [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]\n",
      " [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      " ----- 计算时间 = 0.0ms\n",
      "elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]\n",
      " ----- 计算时间 = 0.0ms\n",
      "gdot = [23.13842153 22.91922857 14.70687609]\n",
      " ----- 计算时间 = 0.0ms\n"
     ]
    }
   ],
   "source": [
    "x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]\n",
    "x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]\n",
    "\n",
    "### 向量化点积 ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"dot = \" + str(dot) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 向量化外积 ###\n",
    "tic = time.process_time()\n",
    "outer = np.outer(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"outer = \\n\" + str(outer) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 向量化向量元素积 ###\n",
    "tic = time.process_time()\n",
    "mul = np.multiply(x1,x2)\n",
    "toc = time.process_time()\n",
    "print (\"elementwise multiplication = \" + str(mul) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")\n",
    "\n",
    "### 向量化一般点积 ###\n",
    "tic = time.process_time()\n",
    "dot = np.dot(W,x1)\n",
    "toc = time.process_time()\n",
    "print (\"gdot = \" + str(dot) + \"\\n ----- 计算时间 = \" + str(1000*(toc - tic)) + \"ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可能已经注意到了，向量化的实现更简洁更高效。对于更大的向量/矩阵， 运行时间差异会更大。\n",
    "\n",
    "**注意** `np.dot()` 执行 矩阵-矩阵 或 矩阵-向量 的乘法。这与 `np.multiply()` 和 `*` 运算符 (等价于 Matlab/Octave 中的 `.*` )，它执行的是元素间的乘法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 执行 L1 和 L2 损失函数\n",
    "\n",
    "**练习**: 执行 numpy 向量化版本的 L1 损失。你可能发现 `abs(x)` 绝对值函数非常有用。\n",
    "\n",
    "**提示**:\n",
    "- 这个损失被用于评估你模型的性能。你的损失越大，你的预测值 ($ \\hat{y} $) 与真实值 ($y$) 的差距也就越大。在深度学习中，你使用梯度下降等优化函数去训练模型并最小化成本。\n",
    "- L1 损失被定义为:\n",
    "\n",
    "$$\n",
    "L_1(\\hat{y}, y) = \\sum_{i=0}^m|y^{(i)} - \\hat{y}^{(i)}| \\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L1\n",
    "\n",
    "def L1(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- 大小为 m 的向量（预测标签）\n",
    "    y -- 大小为 m 的向量（真实标签）\n",
    "    \n",
    "    Returns:\n",
    "    loss -- 上面定义的 L1 损失函数的值\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.sum(np.abs(yhat - y))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 = 1.1\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L1 = \" + str(L1(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "  <tr>\n",
    "    <td> **L1** </td> \n",
    "    <td> 1.1 </td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**: 执行 numpy 向量化版本的 L2 损失。有几种实现 L2 损失的方法，但你可能会发现 `np.dot()` 函数很有用。\n",
    "\n",
    "**提示**: 若 $x = [x_1, x_2, ..., x_n]$，则 `np.dot(x, x)` = $ \\sum_{j=0}^{n} x_j^{2} $\n",
    "\n",
    "- L2 损失被定义为:\n",
    "\n",
    "$$\n",
    "L_2(\\hat{y},y) = \\sum_{i=0}^m(y^{(i)} - \\hat{y}^{(i)})^2 \\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L2\n",
    "\n",
    "def L2(yhat, y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    yhat -- 大小为 m 的向量（预测标签）\n",
    "    y -- 大小为 m 的向量（真实标签）\n",
    "    \n",
    "    Returns:\n",
    "    loss -- 上面定义的 L1 损失函数的值\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    loss = np.sum(np.dot((y - yhat), (y - yhat)))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 = 0.43\n"
     ]
    }
   ],
   "source": [
    "yhat = np.array([.9, 0.2, 0.1, .4, .9])\n",
    "y = np.array([1, 0, 0, 1, 1])\n",
    "print(\"L2 = \" + str(L2(yhat,y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预期输出**: \n",
    "<table style=\"width:20%\">\n",
    "     <tr> \n",
    "       <td> **L2** </td> \n",
    "       <td> 0.43 </td> \n",
    "     </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "恭喜你完成了这项任务。我们希望这个小小的热身练习可以帮助你在未来的作业中，这将是更令人兴奋和有趣的！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "\n",
    "**你需要记住：**\n",
    "\n",
    "- 在深度学习中向量化是非常重要的。它提供了计算效率和清晰度。\n",
    "- 你已经复习了 L1 和 L2 损失。\n",
    "- -你熟悉许多numpy函数，例如：`np.sum`、`np.dot`、`np.multiply`、`np.maximum` 等……"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XHpfv",
   "launcher_item_id": "Zh0CU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
