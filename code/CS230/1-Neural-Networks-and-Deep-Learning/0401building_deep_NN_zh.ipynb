{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一步一步构建你的深度神经网络\n",
    "\n",
    "欢迎来到你的第四周作业（第二部分的第一部分）！你之前训练过一个双层的神经网络（只有一个隐藏层）。本周，你将构建一个深度神经网络，你想要多少层就有多少层！\n",
    "\n",
    "- 在这个 notebook 中，你将实现构建深度神经网络所需的所有功能。\n",
    "- 在下一个作业中，你将使用这些函数来构建用于图像分类的深度神经网络。\n",
    "\n",
    "\n",
    "**完成这个任务后，你将能够：**\n",
    "- 使用像 ReLU 这样的非线性单位来改进你的模型\n",
    "- 构建一个更深的神经网络（包含 1 个以上的隐藏层）\n",
    "- 实现一个易于使用的神经网络类\n",
    "\n",
    "**符号**：\n",
    "- 上标 $[L]$ 表示与第 $L$ 层相关联的数量。\n",
    "    - 例如：$a^{[L]}$ 是第 $L$ 层的激活函数计算值。$W ^ {[L]} $ 和 $ b ^ {[L]}$ 是第 $L$ 层参数。\n",
    "- 上标 $(i)$ 表示与第 $i$ 个样本相关联的数量。\n",
    "    - 示例：$x^{(i)}$ 为第 $i$ 个训练样本。\n",
    "- 角标 $i$ 表示向量的第 $i$ 项。\n",
    "    - 例如：$a^{[L]}_i$ 表示第 $L$ 层激活函数计算的第 $i$ 项。\n",
    "\n",
    "让我们开始吧！\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 导入相关包\n",
    "\n",
    "让我们首先导入本次作业中需要的所有包。\n",
    "- [numpy](www.numpy.org) 是 Python 中在科学计算上主要的包。\n",
    "- [matplotlib](http://matplotlib.org) 是 Python 中绘制图形的库。\n",
    "- `dnn_utils` 为这个笔记本提供了一些必要的函数。\n",
    "- `testCases` 提供了一些测试用例来评估你的函数的正确性。\n",
    "- `np.random.seed(1)` 用于保持所有随机函数调用的一致性。它将帮助我们给你的作业打分。请不要换种子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases_v2 import *\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # 设置绘图图形的默认大小\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - 作业大纲\n",
    "\n",
    "为了构建你的神经网络，你将实现几个 “辅助函数”。这些辅助函数将用于下一个任务来构建一个两层神经网络和一个 L 层神经网络。你要实现的每个辅助函数都有详细的说明，指导你完成必要的步骤。下面是这个作业的大纲，你将：\n",
    "\n",
    "- 初始化两层网络和 $L$ 层神经网络的参数。\n",
    "- 实现正向传播模块（如下图中的紫色部分所示）。\n",
    "     - 完成一层前向传播步骤的线性部分 （得到 $Z^{[L]}$）\n",
    "     - 我们给你提供激活函数 (ReLU/sigmoid)\n",
    "     - 将前两个步骤合并为一个新的 [LINEAR->ACTIVATION] 前向函数。\n",
    "     - 堆叠 [LINEAR->RELU] 前向函数 L-1 次（用于第 1 层到第 L-1 层），并在最后添加一个 [LINEAR->SIGMOID]（用于最后一层 $L$）。这就得到了一个新的 `L_model_forward` 函数。\n",
    "- 计算损失函数。\n",
    "- 实现反向传播模块（下图中红色部分）。\n",
    "    - 完成单层的反向传播步骤的线性部分。\n",
    "    - 我们给出激活函数的梯度 (relu_backward/sigmoid_backward) \n",
    "    - 将前两步合并到一个新的 [LINEAR->ACTIVATION] 反向函数中。\n",
    "    - 堆叠 [LINEAR->RELU] 后向函数 L-1 次并在新的 `L_model_backward` 函数中添加 [LINEAR->SIGMOID] 后向函数。\n",
    "- 最后，更新参数。\n",
    "\n",
    "<img src=\"images/final outline.png\" style=\"width:800px;height:500px;\">\n",
    "\n",
    "**<caption><center> Figure 1</center></caption>**<br>\n",
    "\n",
    "**注意**：对于每个前向函数，都有一个对应的后向函数。这就是为什么在前向传播模块的每一步都要在缓存中存储一些值的原因。缓存的值对于计算梯度很有帮助。在反向传播模块中，您将使用缓存来计算梯度。这个作业将向您展示如何执行这些步骤。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 初始化\n",
    "\n",
    "我们将编写两个辅助函数来初始化模型的参数。第一个函数将用于初始化两层模型的参数。第二个将把这个初始化过程泛化到 $L$ 层。\n",
    "\n",
    "### 3.1 - 双层神经网络\n",
    "\n",
    "**练习**： 创建并初始化双层神经网络的参数。\n",
    "\n",
    "**指导**：\n",
    "- 该模型的结构如下： *LINEAR -> ReLU -> LINEAR -> SIGMOID*. \n",
    "- 使用带有正确形状的 `np.random.randn(shape)*0.01`，对权重矩阵使用随机初始化。\n",
    "- 使用 `np.zeros(shape)`，将偏差矩阵初始化为 0 矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- 输入层大小\n",
    "    n_h -- 隐藏层大小\n",
    "    n_y -- 输出层大小\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Python 字典, 包含你的参数:\n",
    "                    W1 -- 形状为 (n_h, n_x) 的权重矩阵\n",
    "                    b1 -- 形状为 (n_h, 1) 的偏差矩阵\n",
    "                    W2 -- 形状为 (n_y, n_h) 的权重矩阵\n",
    "                    b2 -- 形状为 (n_y, 1) 的偏差矩阵\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = None\n",
    "    b1 = None\n",
    "    W2 = None\n",
    "    b2 = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parameters = initialize_parameters(2,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**：\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td> [[ 0.01624345 -0.00611756]\n",
    " [-0.00528172 -0.01072969]] </td> \n",
    "  </tr>\n",
    "\n",
    "  <tr>\n",
    "    <td> **b1**</td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2**</td>\n",
    "    <td> [[ 0.00865408 -0.02301539]]</td>\n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td> **b2** </td>\n",
    "    <td> [[ 0.]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - L 层神经网络\n",
    "\n",
    "更深的 $l$ 层神经网络的初始化更复杂，因为有更多的权重矩阵和偏置向量。当完成 `initialize_parameters_deep()` 时，你应该确保每一层之间的尺寸匹配。回想一下，$n^{[l]}$是第 $l$ 层中的单元数。因此，例如，如果输入 $X$ 的大小为 $(12288,209)$（其中有 $m=209$ 个样本），则:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "   <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "   <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "</table>\n",
    "\n",
    "请记住，当我们在 python 中计算 $W X + b$ 时，它执行的是广播。例如，如果：\n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "则 $WX + b$ 将会是：\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**： 实现 $l$ 层神经网络的初始化。 \n",
    "\n",
    "**指导**：\n",
    "- 模型的结构是 *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. 即它有 $L-1$ 层使用 ReLU 激活函数的隐藏层以及具有 sigmoid 激活函数的输出层。\n",
    "- 使用 `np.random.rand(shape) * 0.01` 对权重矩阵使用随机初始化。\n",
    "- 使用 `np.zeros(shape)` 对偏差使用 0 初始化。\n",
    "- 我们将不同层的单元数 $n^{[l]}$ 存储在变量 `layer_dims` 中。例如，上周的 \"Planar Data classification model\" 练习中的 `layer_dims` 为 [2,4,1]：有两个输入值，一个有 4 个隐藏单元的隐藏层，一个有 1 个输出单元的输出层。因此，'W1' 的形状是 (4,2)，'b1' 是 (4,1)，'W2'是 (1,4)，'b2' 是 (1,1)。现在您将其推广到 $L$ 层！\n",
    "- 这是 $L=1$（单层神经网络）的实现。它应该会启发你实现一般情况（$l$ 层神经网络）。\n",
    "\n",
    "```python\n",
    "    if L == 1:\n",
    "        parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "        parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- Python数组 (列表)，包含网络中每一层的尺寸\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- Python字典, 包含你的参数 \"W1\", \"b1\", …, \"WL\", \"bL\":\n",
    "                    Wl -- 形状为 (layer_dims[l], layer_dims[l-1]) 的权重矩阵\n",
    "                    bl -- 形状为 (layer_dims[l], 1) 的偏差向量\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # 网络中的层数\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = None\n",
    "        parameters['b' + str(l)] = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**：\n",
    "       \n",
    "<table style=\"width:80%\">\n",
    "  <tr>\n",
    "    <td> **W1** </td>\n",
    "    <td>[[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
    " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
    " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
    " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b1** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**W2** </td>\n",
    "    <td>[[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
    " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
    " [-0.00768836 -0.00230031  0.00745056  0.01976111]]</td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td>**b2** </td>\n",
    "    <td>[[ 0.]\n",
    " [ 0.]\n",
    " [ 0.]]</td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - 前向传播模块\n",
    "\n",
    "### 4.1 - 线性前向\n",
    "现在您已经初始化了参数，您将执行正向传播模块。您将从实现一些基本函数开始，稍后在实现模型时将使用这些函数。你将按照下面的顺序完成三个函数:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION 这里的 ACTIVATION 激活函数要么是 ReLU 要么是 Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID（整个模型）\n",
    "\n",
    "线性前向模块（对所有样本进行向量化）计算如下方程：\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "这里的 $A^{[0]} = X$. \n",
    "\n",
    "**练习**： 构建前向传播的线性部分。\n",
    "\n",
    "**提示**：\n",
    "这个单位的数学表示为 $Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}$。你可能也会发现 `np.dot()` 很有用。如果尺寸不匹配，打印 `W.shape`，可能有帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    实现一层前向传播的线性部分。\n",
    "\n",
    "    Arguments:\n",
    "    A -- 来自前一层 (或输入数据) 的激活值: (前一层的大小，示例数量)\n",
    "    W -- 权重矩阵: 形状为 (当前层的大小, 前一层的大小) 的 numpy 数组\n",
    "    b -- 偏差向量: 形状为 (当前层的大小, 1) 的 numpy 数组\n",
    "\n",
    "    Returns:\n",
    "    Z -- 激活函数的输入也被称为预激活参数  \n",
    "    cache -- 一个包含 \"A\", \"W\" 和 \"b\" 的 Python 字典; 存储起来以便用于高效地计算反向传递\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**：\n",
    "\n",
    "<table style=\"width:35%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td> **Z** </td>\n",
    "    <td> [[ 3.26295337 -1.23429987]] </td> \n",
    "  </tr>\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - 线性正向激活函数\n",
    "\n",
    "在这个 notebook 中，你将使用两个激活函数：\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$. 我们已经为您提供了 `sigmoid` 函数。这个函数返回 **2** 个元素：激活值 \"`a`\" 和一个包含 \"`Z`\"（这就是我们将输入到相应的反向函数的内容）的 \"`cache`\"。要使用它，你可以调用：\n",
    "\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: ReLu 的数学公式为 $A = ReLU(Z) = max(0, Z)$. 我们已经为您提供了 `ReLU` 函数。这个函数返回 **2** 个元素：激活值 \"`A`\" 和一个包含 \"`Z`\"（这就是我们将输入到相应的反向函数的内容）\"`cache`\"。要使用它，你可以调用：\n",
    "\n",
    "\n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了方便起见，我们将把两个函数 (LINEAR 线性函数和 ACTIVATION 激活函数) 归为一个函数 (LINEAR->ACTIVATION)。因此，您将实现一个函数，该函数执行线性前向步骤，然后是激活前向步骤。\n",
    "\n",
    "**练习**：使用带有正确的激活函数的 `linear_forward()` 实现 *LINEAR->ACTIVATION* 层的正向传播。\n",
    "\n",
    "数学关系为：$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ 其中激活函数 \"g\" 可以是 `sigmoid()` 或 `relu()`。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    实现 LINEAR->ACTIVATION 层的正向传播\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- 来自前一层 (或输入数据) 的激活值: (前一层大小, 样本数量)\n",
    "    W -- 权重矩阵: 形状为 (当前层的大小, 前一层的大小) 的 numpy 数组\n",
    "    b -- 偏差向量: 形状为 (当前层的大小, 1) 的 numpy 数组\n",
    "    activation -- 该层中使用的激活函数，存储为文本字符串: \"sigmoid\" 或 \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- 激活函数的输出，也称为激活后值\n",
    "    cache -- 一个包含 \"linear_cache\" 和 \"activation_cache\" 的 Python 字典; 存储用于高效地计算反向传递\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # 输入: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = None\n",
    "        A, activation_cache = None\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # 输入: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = None\n",
    "        A, activation_cache = None\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**：\n",
    "       \n",
    "<table style=\"width:35%\">\n",
    "  <tr>\n",
    "    <td> **With sigmoid: A ** </td>\n",
    "    <td > [[ 0.96890023  0.11013289]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **With ReLU: A ** </td>\n",
    "    <td > [[ 3.43896131  0.        ]]</td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意**：在深度学习中，\"[LINEAR->ACTIVATION]\" 计算被计算为神经网络中的一层，而不是两层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - L 层模型\n",
    "\n",
    "For even more convenience when implementing the $L$-layer Neural Net, you will need a function that replicates the previous one (`linear_activation_forward` with RELU) $L-1$ times, then follows that with one `linear_activation_forward` with SIGMOID.\n",
    "\n",
    "为了在实现 $L$ 层神经网络时更方便，您将需要一个函数来复制前一个函数（使用 ReLU 函数的 `linear_activation_forward()`）$L-1$ 次，然后使用一次使用 SIGMOID 函数的 `linear_activation_forward()`。\n",
    "\n",
    "<img src=\"images/model_architecture_kiank.png\" style=\"width:600px;height:300px;\">\n",
    "<center> **Figure 2** : *[LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model</center>\n",
    "\n",
    "**练习**： 实现上述模型的正向传播。\n",
    "\n",
    "**指导**： 在下面的代码中，变量 `AL` 将表示 $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$。（有时也被称为 `Yhat`，即是 $\\hat{Y}$）\n",
    "\n",
    "**Tips**:\n",
    "- 使用你之前编写的函数\n",
    "- 使用 for 循环复制 [LINEAR->RELU] $(L-1)$ 次\n",
    "- 不要忘记跟踪 \"cache\" 列表中的缓存。你可以使用 `list.append(c)` 给 `list` 添加一个新值 `c`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    实现 [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID 计算的正向传播\n",
    "    \n",
    "    Arguments:\n",
    "    X -- 输入数据，形状为 (输入值大小, 样本数量) 的 numpy 数组\n",
    "    parameters -- initialize_parameters_deep() 的输出\n",
    "    \n",
    "    Returns:\n",
    "    AL -- 上一个 post-activation 的值\n",
    "    caches -- 缓存列表包含:\n",
    "                每个 linear_relu_forward() 的缓存 (一共有 L-1 个, 索引从 0 到 L-2)\n",
    "                linear_sigmoid_forward() 的缓存 (有一个, 索引为 L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # 神经网络的层数\n",
    "    \n",
    "    # 师兄 [LINEAR -> RELU]*(L-1). 添加 \"cache\" 到 \"caches\" 列表.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = None\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # 实现 LINEAR -> SIGMOID. 添加 \"cache\" 到 \"caches\" 列表.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = None\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"width:40%\">\n",
    "  <tr>\n",
    "    <td> **AL** </td>\n",
    "    <td > [[ 0.17007265  0.2524272 ]]</td> \n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td> **Length of caches list ** </td>\n",
    "    <td > 2</td> \n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你得到了一个完整的正向传播模型，它接受输入 X，并输出一个包含预测结果的行向量 $ a ^{[L]}$。它还将所有中间值记录在 \"cache\" 中。使用 $A^{[L]}$，你可以计算预测的成本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 成本函数\n",
    "\n",
    "现在你将实现正向和反向传播。你需要计算成本，因为你想检查你的模型是否真的在学习。\n",
    "\n",
    "**练习**： 使用下面的公式计算交叉熵代价 $J$：\n",
    "\n",
    "$$-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} [y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)] \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    实现公式(7) 定义的成本函数。\n",
    "\n",
    "    Arguments:\n",
    "    AL -- 对应于你的标签预测的概率向量, 形状为 (1, 样本数量)\n",
    "    Y -- true \"标签\" 向量 (例如: 包含 0 是 non-cat, 1 是 cat), 形状为 (1, 样本数量)\n",
    "\n",
    "    Returns:\n",
    "    cost -- 交叉熵损失\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # 计算 aL 和 y 的损失。\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # 为了确保你的成本的形状符合我们的预期 (e.g. 将 [[17]] 转换为 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>**cost** </td>\n",
    "        <td> 0.41493159961539694</td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - 后向传播模块\n",
    "\n",
    "就像前向传播一样，你将实现反向传播的辅助函数。请记住，反向传播用于计算损失函数相对于参数的梯度。\n",
    "\n",
    "**提示**：\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:650px;height:250px;\">\n",
    "\n",
    "<center>**Figure 3** : Forward and Backward propagation for *LINEAR->RELU->LINEAR->SIGMOID*</center>\n",
    "\n",
    "*<center>The purple blocks represent the forward propagation, and the red blocks represent the backward propagation.</center>*\n",
    "\n",
    "<!-- \n",
    "For those of you who are expert in calculus (you don't need to be to do this assignment), the chain rule of calculus can be used to derive the derivative of the loss $\\mathcal{L}$ with respect to $z^{[1]}$ in a 2-layer network as follows:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "In order to calculate the gradient $dW^{[1]} = \\frac{\\partial L}{\\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial W^{[1]}}$. During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted.\n",
    "\n",
    "Equivalently, in order to calculate the gradient $db^{[1]} = \\frac{\\partial L}{\\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \\times \\frac{\\partial z^{[1]} }{\\partial b^{[1]}}$.\n",
    "\n",
    "This is why we talk about **backpropagation**.\n",
    "!-->\n",
    "\n",
    "现在，与正向传播类似，你将通过三个步骤构建反向传播：\n",
    "- 线性反向\n",
    "- 线性计算 ->  后向 ACTIVATION，其中 ACTIVATION 计算的是 ReLU 或 sigmoid 激活函数的导数\n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> 后向 SIGMOID（整个模型）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - 线性后向\n",
    "\n",
    "For layer $l$, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$ (followed by an activation).\n",
    "\n",
    "Suppose you have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$.\n",
    "\n",
    "<img src=\"images/linearback_kiank.png\" style=\"width:250px;height:300px;\">\n",
    "<caption><center> **Figure 4** </center></caption>\n",
    "\n",
    "The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$ are computed using the input $dZ^{[l]}$.Here are the formulas you need:\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**： 使用上面 3 个公式去实现 `linear_backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = None\n",
    "    db = None\n",
    "    dA_prev = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**： \n",
    "\n",
    "<table style=\"width:90%\">\n",
    "  <tr>\n",
    "    <td> **dA_prev** </td>\n",
    "    <td > [[ 0.51822968 -0.19517421]\n",
    " [-0.40506361  0.15255393]\n",
    " [ 2.37496825 -0.89445391]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td> **dW** </td>\n",
    "    <td > [[-0.10076895  1.40685096  1.64992505]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td> **db** </td>\n",
    "    <td> [[ 0.50629448]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - 线性后向激活函数\n",
    "\n",
    "Next, you will create a function that merges the two helper functions: **`linear_backward`** and the backward step for the activation **`linear_activation_backward`**. \n",
    "\n",
    "To help you implement `linear_activation_backward`, we provided two backward functions:\n",
    "- **`sigmoid_backward`**: Implements the backward propagation for SIGMOID unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "- **`relu_backward`**: Implements the backward propagation for RELU unit. You can call it as follows:\n",
    "\n",
    "```python\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "```\n",
    "\n",
    "If $g(.)$ is the activation function, \n",
    "`sigmoid_backward` and `relu_backward` compute $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "**练习**： Implement the backpropagation for the *LINEAR->ACTIVATION* layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = None\n",
    "        dA_prev, dW, db = None\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = None\n",
    "        dA_prev, dW, db = None\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用 sigmoid 函数的预计输出**：\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td >[[ 0.11017994  0.01105339]\n",
    " [ 0.09466817  0.00949723]\n",
    " [-0.05743092 -0.00576154]] </td> \n",
    "\n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.10266786  0.09778551 -0.01968084]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.05729622]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**使用 ReLU 函数的预计输出**：\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <td > dA_prev </td> \n",
    "           <td > [[ 0.44090989  0.        ]\n",
    " [ 0.37883606  0.        ]\n",
    " [-0.2298228   0.        ]] </td> \n",
    "\n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > dW </td> \n",
    "           <td > [[ 0.44513824  0.37371418 -0.10478989]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db </td> \n",
    "           <td > [[-0.20837892]] </td> \n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 - L 模型后向\n",
    "\n",
    "Now you will implement the backward function for the whole network. Recall that when you implemented the `L_model_forward` function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the `L_model_backward` function, you will iterate through all the hidden layers backward, starting from layer $L$. On each step, you will use the cached values for layer $l$ to backpropagate through layer $l$. Figure 5 below shows the backward pass. \n",
    "\n",
    "\n",
    "<img src=\"images/mn_backward.png\" style=\"width:450px;height:300px;\">\n",
    "<caption><center>  **Figure 5** : Backward pass  </center></caption>\n",
    "\n",
    "** Initializing backpropagation**:\n",
    "To backpropagate through this network, we know that the output is, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Your code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "To do so, use this formula (derived using calculus which you don't need in-depth knowledge of):\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "```\n",
    "\n",
    "You can then use this post-activation gradient `dAL` to keep going backward. As seen in Figure 5, you can now feed in `dAL` into the LINEAR->SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a `for` loop to iterate through all the other layers using the LINEAR->RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula : \n",
    "\n",
    "$$grads[\"dW\" + str(l)] = dW^{[l]}\\tag{15} $$\n",
    "\n",
    "For example, for $l=3$ this would store $dW^{[l]}$ in `grads[\"dW3\"]`.\n",
    "\n",
    "**练习**： Implement backpropagation for the *[LINEAR->RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID* model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ...\n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = None\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = None\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(L - 1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = None\n",
    "        dA_prev_temp, dW_temp, db_temp = None\n",
    "        grads[\"dA\" + str(l + 1)] = None\n",
    "        grads[\"dW\" + str(l + 1)] = None\n",
    "        grads[\"db\" + str(l + 1)] = None\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**：\n",
    "\n",
    "<table style=\"width:60%\">\n",
    "  \n",
    "  <tr>\n",
    "    <td > dW1 </td> \n",
    "           <td > [[ 0.41010002  0.07807203  0.13798444  0.10502167]\n",
    " [ 0.          0.          0.          0.        ]\n",
    " [ 0.05283652  0.01005865  0.01777766  0.0135308 ]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > db1 </td> \n",
    "           <td > [[-0.22007063]\n",
    " [ 0.        ]\n",
    " [-0.02835349]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "  <td > dA1 </td> \n",
    "           <td > [[ 0.          0.52257901]\n",
    " [ 0.         -0.3269206 ]\n",
    " [ 0.         -0.32070404]\n",
    " [ 0.         -0.74079187]] </td> \n",
    "\n",
    "  </tr> \n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 - 更新参数\n",
    "\n",
    "In this section you will update the parameters of the model, using gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**练习**：实现 `update_parameters()` 来使用梯度下降更新你的参数。\n",
    "\n",
    "**指导**：在每个 $W^{[l]}$ and $b^{[l]}$ for $l = 1, 2, ..., L$ 上使用梯度下降更新参数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = None\n",
    "        parameters[\"b\" + str(l+1)] = None\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**预计输出**：\n",
    "\n",
    "<table style=\"width:100%\"> \n",
    "    <tr>\n",
    "    <td > W1 </td> \n",
    "           <td > [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
    " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
    " [-1.0535704  -0.86128581  0.68284052  2.20374577]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > b1 </td> \n",
    "           <td > [[-0.04659241]\n",
    " [-1.28888275]\n",
    " [ 0.53405496]] </td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > W2 </td> \n",
    "           <td > [[-0.55569196  0.0354055   1.32964895]]</td> \n",
    "  </tr> \n",
    "  <tr>\n",
    "    <td > b2 </td> \n",
    "           <td > [[-0.84610769]] </td> \n",
    "  </tr> \n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - 结论\n",
    "\n",
    "祝贺你实现了构建深度神经网络所需的所有功能！\n",
    "\n",
    "我们知道这是一个漫长的任务，但继续前进，它只会变得更好。作业的下一部分比较容易。\n",
    "\n",
    "在下一个作业中，你将把所有这些放在一起构建两个模型：\n",
    "- 两层神经网络\n",
    "- L 层神经网络\n",
    "\n",
    "实际上，您将使用这些模型对猫和非猫图像进行分类！"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
